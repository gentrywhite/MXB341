<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<meta name="progressive" content="false" />
<meta name="allow-skip" content="false" />

<title>MXB341 Worksheet 3 - Maximum Likelihood Estimators</title>


<!-- highlightjs -->
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>

<!-- taken from https://github.com/rstudio/rmarkdown/blob/67b7f5fc779e4cfdfd0f021d3d7745b6b6e17149/inst/rmd/h/default.html#L296-L362 -->
<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("section-TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>
<!-- end tabsets -->


<link rel="stylesheet" href="css/QUT.css" type="text/css" />
<link rel="stylesheet" href="css/QUTtutorial.css" type="text/css" />

</head>

<body>



<div class="pageContent band">
<div class="bandContent page">

<div class="topics">

<p style="margin-bottom:0.6cm;"></p>

<a><img src = "images/logo.png" height="100px" 
          alt = "MXB341 Blackboard Page"><span style="font-size:40px;color:#00407a"> &thinsp; MXB341 Statistical Inference</span></a>

<p style="margin-bottom:1cm;"></p>
<div class="quote">
<p>Far better an approximate answer to the <em>right</em> question, which is often vague, than an <em>exact</em> answer to the wrong question, which can always be made precise.</p>
<p>—John Tukey (1962)<a href="#section-fn1" class="footnote-ref" id="section-fnref1"><sup>1</sup></a></p>
</div>
<div id="section-week-3-properties-of-mles-deviance-and-exponential-families" class="section level1">
<h1>Week 3: Properties of MLEs, Deviance, and Exponential Families</h1>
<p>Maximum likelihood estimators, while not the only means (or in some cases the preferred) of estimating the values of parameters, are ubiquitous. The reason for their popularity is due to their desirable properties, both inherently and asymptotically. These properties ensure that the maximum likelihood estimators satisfy the criteria we have for estimators: unbiasedness, consistency, and efficiency.</p>
<div id="section-maximum-likelihood-estimators" class="section level2 tabset tabset-pills boxed">
<h2 class="tabset tabset-pills">Maximum Likelihood Estimators</h2>
<p>Maximum likelihood estimators are the values that maximise the likelihood function. The likelihood function and its behaviour define the estimators and their properties.</p>
<div id="section-the-likelihood-function" class="section level3">
<h3>The Likelihood Function</h3>
<p style="margin-bottom:0.3cm;">
</p>
<p>Given a probability density function<a href="#section-fn2" class="footnote-ref" id="section-fnref2"><sup>2</sup></a> <span class="math inline">\(f(x;\theta)\)</span> and a sample <span class="math inline">\(\mathbf{x}=x_1,x_2,\ldots,x_n\)</span> from <span class="math inline">\(f(x;\theta)\)</span> the likelihood function is <span class="math display">\[
L(\theta|\mathbf{x})=\prod_{i=1}^nf(x_i;\theta)
\]</span> a function of the parameter <span class="math inline">\(\theta\)</span> conditioned on the random sample <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>In many cases, it is easier to work with the log-likelihood function defined as <span class="math display">\[
\begin{align}
l(\theta|\mathbf{x})&amp;=\log\left(\prod_{i=1}^nf(x_i;\theta)\right)\\
&amp;=\sum_{i=1}^n
\log(f(x_i;\theta)).
\end{align}
\]</span></p>
<div id="section-the-score-function" class="section level4">
<h4>The Score Function</h4>
<p style="margin-bottom:0.3cm;">
</p>
<p>The score function is the first derivative (or gradient in the multivariate case) of the log-likelihood function with respect to the parameter(s). By definition, the score function evaluated at any point in the parameter space is the sensitivity of the log-likelihood function with respect to infinitesimal changes to the parameter(s).</p>
<p>For a log-likelihood function <span class="math inline">\(l(\theta|\mathbf{x})\)</span> the score function is <span class="math display">\[
S(\theta)=\frac{d}{d\theta}l(\theta|\mathbf{x}).
\]</span> If the parameter <span class="math inline">\(\theta\)</span> is a vector, i.e. <span class="math inline">\(\boldsymbol{\theta}=(\theta_1,\theta_2,\ldots,\theta_p)\)</span> then the score function is a gradient, or vector of partial derivatives <span class="math display">\[
S(\boldsymbol{\theta})=\left(\frac{\partial}{\partial\theta_1}l(\boldsymbol{\theta}|\mathbf{x}),\frac{\partial}{\partial\theta_2}l(\boldsymbol{\theta}|\mathbf{x}),\ldots,\frac{\partial}{\partial\theta_p}l(\boldsymbol{\theta}|\mathbf{x})\right).
\]</span></p>
<p>The maximum likelihood estimator <span class="math inline">\(\hat{\theta}\)</span> is defined such <span class="math display">\[
S(\hat{\theta})=0
\]</span> or equivalently the expression <span class="math display">\[
\hat{\theta}=\left\{\theta\in\Theta:\frac{d}{d\theta}l(\theta|\mathbf{x})\bigg|_{\theta=\hat{\theta}}= 0\right\}
\]</span> gives a direct approach to finding the maximum likelihood estimator.</p>
</div>
</div>
<div id="section-the-hessian-and-fisher-information" class="section level3">
<h3>The Hessian and Fisher Information</h3>
<p style="margin-bottom:0.3cm;">
</p>
<p>The Hessian is the matrix of second partial derivatives of the log-likelihood. For a two-parameter log-likelihood it is: <span class="math display">\[ \mathcal{H}(\boldsymbol{\theta}) = \left[\begin{array}{cc} 
\frac{\partial^{2} \ell}{\partial \theta_{1}^{2}} &amp; \frac{\partial^{2} \ell}{\partial \theta_{1}\partial \theta_{2}}\\
\frac{\partial^{2} \ell}{\partial \theta_{1}\partial \theta_{2}} &amp; \frac{\partial^{2} \ell}{\partial \theta_{2}^{2}}
\end{array}\right] \]</span> If the Hessian is negative semi-definite at the MLE values, then you have found a (local) maximum of the likelihood.</p>
<div id="section-expected-fisher-information-matrix" class="section level4">
<h4>Expected (Fisher) information matrix</h4>
<p>The Expected Fisher Information Matrix measures the amount of information the observations carry about the unknown parameter you are estimating. The expected information for <span class="math inline">\(n\)</span> iid observations is equal to <span class="math display">\[
\mathcal{I}_{n}(\boldsymbol{\theta}) = -E\left[\mathcal{H}(\boldsymbol{\theta})\vert~\boldsymbol{\theta})\right]
\]</span> where the expectation is with respect to the observations (treated as random variables)<a href="#section-fn3" class="footnote-ref" id="section-fnref3"><sup>3</sup></a>. Often, and somewhat confusingly, the expected information matrix just refers to a singular observation <span class="math inline">\(y\)</span> with pmf or pdf <span class="math inline">\(f(y|\boldsymbol{\theta})\)</span>. In this case, for two parameters, the (single observation) expected information is <span class="math display">\[
\mathcal{I}(\boldsymbol{\theta}) = -
\begin{pmatrix}
E\left(\frac{\partial^2}{\partial \theta_{1}^{2} } \log f(y~\vert~\boldsymbol{\theta}) \right) &amp; E\left(\frac{\partial^2}{\partial \theta_{1} \partial \theta_{2} } \log f(y~\vert~\boldsymbol{\theta})\right) \\
E\left( \frac{\partial^2}{\partial \theta_{1} \partial \theta_{2} } \log f(y~\vert~\boldsymbol{\theta})\right) &amp; E\left( \frac{\partial^2}{\partial \theta_{2}^{2} } \log f(y~\vert~\boldsymbol{\theta}) \right)
\end{pmatrix}
\]</span> As before, the expectation is with respect to the random variable <span class="math inline">\(y\)</span>. Note that <span class="math inline">\(\mathcal{I}_{n}(\boldsymbol{\theta}) = n \mathcal{I}(\boldsymbol{\theta})\)</span>.</p>
</div>
<div id="section-observed-fisher-information-matrix" class="section level4">
<h4>Observed (Fisher) information matrix</h4>
<p>is the sample-based version of the Fisher information matrix. It is equal to <span class="math display">\[
\mathcal{J}_{n}(\boldsymbol{\theta}) = -\mathcal{H}\left(\boldsymbol{\theta}\right)
\]</span> Note that it is often evaluated at the MLE <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>, i.e. <span class="math inline">\(\mathcal{J}_{n}(\hat{\boldsymbol{\theta}})\)</span>, and a scaled version is also often used: <span class="math inline">\(\mathcal{J}(\boldsymbol{\theta}) = n^{-1}\mathcal{J}_{n}(\boldsymbol{\theta})\)</span>.</p>
<p>To find the MLEs when there are multiple parameters, we usually need to use numerical methods, but in general, the same ideas apply as the univariate case. That is, (1) find a potential maximum, and (2) use a graph or the Hessian to determine if it is indeed the local (and hopefully global) maximum.</p>
</div>
</div>
</div>
<div id="section-the-sufficency-and-efficiency-of-mles" class="section level2 tabset tabset-pills boxed">
<h2 class="tabset tabset-pills">The Sufficency and Efficiency of MLEs</h2>
<div id="section-sufficiency" class="section level3">
<h3>Sufficiency</h3>
<p style="margin-bottom:0.3cm;">
</p>
<p>A statistic (i.e. some quantity computed from observed data) is sufficient for some model or probability distribution and their unknown parameter(s) if there is no other statistic which contains more information about the parameter.</p>
<div class="boxed">
<p>Given a sample <span class="math inline">\(\mathbf{x}=(x_1,x_2,\ldots,x_n)\)</span> of the random variable <span class="math inline">\(X\sim f(x;\theta)\)</span> the statistic <span class="math inline">\(T(\mathbf{x})\)</span> is <em>sufficient</em> for <span class="math inline">\(\theta\)</span> if <span class="math display">\[
f(\mathbf{x}|T(\mathbf{x}),\theta)=f(\mathbf{x}|T(\mathbf{x}))
\]</span> the probability distribution of the data <span class="math inline">\(\mathbf{x}\)</span> conditioned on the statistic <span class="math inline">\(T(\mathbf{x})\)</span> is independent of <span class="math inline">\(\theta\)</span>.</p>
</div>
<p>More simply put, a sufficient statistic contains all the information in the data about the parameter value.</p>
<p>The <em>Neyman-Fisher</em> factorisation theorem provides a convenient expression for identifying sufficient statistics.</p>
<div class="boxed">
<p><strong>The Neyman-Fisher Factorisation Theorem</strong><br />
If a given probability density <span class="math inline">\(f(x;\theta)\)</span> the statistic <span class="math inline">\(T(x)\)</span> is a sufficient statistic if and only the non-negative functions <span class="math inline">\(g(\cdot)\)</span> and <span class="math inline">\(h(\cdot)\)</span> can be found such that <span class="math display">\[
f(x;\theta)=h(x)g(T(x),\theta).
\]</span></p>
</div>
<p>The Neyman-Pearson Factorisation theorem is stated in terms of a probability distribution but extends to the likelihood and log-likelihood. This extension implies that the relationship between the parameter and the data is sufficient statistics; hence MLEs are functions of sufficient statistics.</p>
</div>
<div id="section-efficiency" class="section level3">
<h3>Efficiency</h3>
<p style="margin-bottom:0.3cm;">
</p>
<p>The efficiency of <span class="math inline">\(\tilde{\theta}\)</span> an unbiased estimator of the parameter <span class="math inline">\(\theta\)</span> is <span class="math display">\[
e(\tilde{\theta})=\frac{\mathcal{I}_n^{-1}(\theta)}{\operatorname{Var}(\tilde{\theta})}
\]</span> Using the Cramér–Rao Lower Bound, we can prove that <span class="math display">\[
e(\tilde{\theta})\leq 1.
\]</span></p>
<div class="boxed">
<p><strong>The Rao-Blackwell Theorem</strong><br />
</p>
<p>Let <span class="math inline">\(\hat{\theta}\)</span> be an unbiased estimator of the parameter <span class="math inline">\(\theta\)</span> such that <span class="math display">\[
\operatorname{E}\left(\hat{\theta}^2\right)&lt;\infty
\]</span> and let <span class="math inline">\(T\)</span> be a sufficent statistic for <span class="math inline">\(\theta\)</span> and define <span class="math inline">\(\tilde{\theta}=\operatorname{E}(\hat{\theta}|T)\)</span>.</p>
<p>Then <span class="math display">\[
\operatorname{E}\left[(\tilde{\theta}-\theta)^2
\right]\leq
\operatorname{E}\left[(\hat{\theta}-\theta)^2
\right]
\]</span></p>
</div>
<p>In other words, conditioning on sufficient statistics improves estimators’ efficiency, unless the estimator has already achieved the Cramér–Rao Lower Bound.</p>
</div>
</div>
<div id="section-asymptotic-properties-of-mles" class="section level2 tabset tabset-pills boxed">
<h2 class="tabset tabset-pills">Asymptotic Properties of MLEs</h2>
<p>Estimators are functions of data, which are random variables, which implies that estimators are also random variables. We can use this fact to make inference or probabilistic statements about estimators and their values, the strength of our belief in those values, and ultimately perform statistical tests based on these estimators and explore decision making under uncertainty.</p>
<div id="section-convergence-of-the-mle" class="section level3">
<h3>Convergence of the MLE</h3>
<p style="margin-bottom:0.3cm;">
</p>
<p>The first step to understanding the estimators’ asymptotic behaviour is to look at some of the theorems describing the convergence behaviours of estimators to understand the formal approaches underpinning their behaviours.</p>
<p><strong>The Central Limit Theorem</strong></p>
<p>The Central Limit Theorem is shown in elementary statistics units to justify the use of <span class="math inline">\(Z\)</span> or <span class="math inline">\(t\)</span>-tests for sample means, and as stated, is an example of <em>convergence in distribution</em> <span class="math display">\[
\sqrt{n}\left(\bar{x}-\operatorname{E}(X)\right)\stackrel{d}{\rightarrow}N(0,\sigma^2)
\]</span> or that for sufficiently large samples, we can assume that the sample mean follows a Gaussian distribution with the population mean and variance. This convergence is a weak form of convergence but is useful in establishing a basis for inference on the sample mean.</p>
<p><strong>The Law of Large Numbers</strong></p>
<p>The Law of Large Numbers with respect to the sample mean is a stronger statement of convergence, as an example of <em>convergence in probability</em> <span class="math display">\[
Pr(|\bar{x}-\operatorname{E}(X)|&lt;\epsilon)\rightarrow 0, n\rightarrow\infty, \forall \epsilon.
\]</span> In other words, if <span class="math inline">\(X\)</span> is a random variable from a probability distribution with <span class="math inline">\(\operatorname{E}(X)\)</span> then for a random sample <span class="math inline">\(\mathbf{x}=x_1,x_2,\ldots,x_n\)</span> from the probability distribution of <span class="math inline">\(X\)</span>, the sample mean <span class="math inline">\(\bar{x}\)</span> approaches <span class="math inline">\(\operatorname{E}(X)\)</span> as <span class="math inline">\(n\)</span> approaches infinity.</p>
<p>The Law of Large Numbers describes the behaviour of the sample mean of samples from a random variable with respect to the expected value of that random variable. But we will see that we can use the Law of Large Numbers to define consistency for any estimator, not just the sample mean.</p>
<p>The Law of Large Numbers provides more formal proof of the consistency of the MLE.</p>
</div>
<div id="section-asymptotic-normality-of-the-mle" class="section level3">
<h3>Asymptotic Normality of the MLE</h3>
<p style="margin-bottom:0.3cm;">
</p>
<p>We can state the proposition that the maximum likelihood estimator is distributed asymptotically following a Gaussian (Normal) probability density function as <span class="math display">\[
\sqrt{n}(\hat{\theta}-\theta_0)\rightarrow N\left(0,\frac{1}{\mathcal{I}(\theta_0)}\right)
\]</span> where <span class="math inline">\(\hat{\theta}\)</span> is the maximum likelihood estimator of the unknown parameter <span class="math inline">\(\theta_0\)</span>, and <span class="math inline">\(\mathcal{I}(\theta_0)\)</span> is the Fisher Information for <span class="math inline">\(\theta\)</span> evaluated at <span class="math inline">\(\theta_0\)</span>.</p>
<p>The proof that the maximum likelihood estimator <span class="math inline">\(\hat{\theta}\)</span> is both consistent and <span class="math display">\[
\sqrt{n}(\hat{\theta}-\theta_0)\stackrel{d}{\rightarrow}N\left(0,\frac{1}{\mathcal{I}(\theta_0)}\right)
\]</span> converges in distribution to a Gaussian distribution with a mean of <span class="math inline">\(\theta_0\)</span> and variance of <span class="math inline">\(\mathcal{I}(\theta_0)^{-1}\)</span> require some detail to work out. Still, the results are useful in understanding the ultimate behaviour of the maximum likelihood estimators. These results allow us to make probabilistic statements about the maximum likelihood estimator and formal statistical inference.</p>
</div>
</div>
<div id="section-exponential-families" class="section level2 tabset tabset-pills boxed">
<h2 class="tabset tabset-pills">Exponential families</h2>
<p>The exponential families of distributions refer to a rather large and ubiquitous set of probability distributions written in a common general form. We use the term “families” to denoting each distribution as a “family”, i.e. the Gaussian distribution is a family of all possible Gaussian distributions and is a member of the exponential families. The exponential families and their common form have several useful properties that facilitate many inference tasks, including parameter estimation and inference and model construction and evaluation.</p>
<p>Members of the exponential family of distributions are probability distributions whose support is independent of their parameters, and we can write in a common form <span class="math display">\[
f(x;\boldsymbol{\theta})=h(x)\exp\left(\eta(\boldsymbol{\theta})T(x)-A(\boldsymbol{\theta}
\right).
\]</span></p>
<ul>
<li><span class="math inline">\(h(x)&gt;0\)</span> is a function of the data</li>
<li><span class="math inline">\(T(x)\)</span> is the sufficient statistic. For the likelihood function with samples <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span> the sufficient statistic is <span class="math inline">\(\sum_{i=1}^nT(x_i)\)</span>.<br />
</li>
<li>If <span class="math inline">\(\eta(\theta)=\theta=\eta\)</span>, <span class="math inline">\(\eta\)</span> is called the <em>natural parameter</em> and the family is said to be in the canonical form</li>
</ul>
<p><span class="math display">\[
f(x;\boldsymbol{\theta})=h(x)\exp\left(\eta T(x)-A(\eta)\right)
\]</span></p>
<ul>
<li>If <span class="math inline">\(\eta(\theta)=\eta\)</span> and <span class="math inline">\(T(x)=x\)</span> the familiy is called a <em>natural exponential</em> family, and the <em>natural parameter space</em> is</li>
</ul>
<p><span class="math display">\[
\{\eta:f(x;\theta)&lt;\infty\}.
\]</span></p>
<ul>
<li>The function <span class="math inline">\(A(\theta\)</span> or <span class="math inline">\(A(\eta)\)</span> is defined after finding the other components and is called the log-partition function because it is the natural log of the normalising constant</li>
</ul>
<p><span class="math display">\[
A(\eta)=\log\left(
\int_Xh(x)exp\left(\eta(\theta)T(x)\right)dx
\right).
\]</span> There are numerous examples of exponential families, and the concept of exponential families is fundamental to generalised linear models and Bayesian statistics.</p>
</div>
<div id="section-deviance-and-information-criteria" class="section level2 tabset tabset-pills boxed">
<h2 class="tabset tabset-pills">Deviance and information criteria</h2>
<p>Deviance is a goodness of fit measure for statistical models, comparing a fitted model to a theoretical “best fit” or saturated model <span class="math display">\[
D(\mathbf{x},\theta)=2\left\{l(\theta_s|\mathbf{x})-l(\hat{\theta}|\mathbf{x})\right\}
\]</span> where <span class="math inline">\(\theta_s\)</span> is the saturated model estimates of the parameters, and <span class="math inline">\(\hat{\theta}\)</span> are the proposed model parameter estimates (typically the MLE).</p>
<p>While deviance is a relative measure, because the saturated log-likelihood may not be equal to <span class="math inline">\(0\)</span>, it can be useful for comparing different models.</p>
<div id="section-akaike-information-criteria" class="section level3">
<h3>Akaike Information Criteria</h3>
<p style="margin-bottom:0.3cm;">
</p>
<p>The Akaike Information Criteria (AIC) estimates the predictive error for a model that includes a penalty term based on <span class="math inline">\(k\)</span> the number of estimated parameters in the model. The (AIC) is twice the number of estimated parameters minus twice the maximum of the log-likelihood (i.e. evaluated at the MLE).</p>
<p><span class="math display">\[
\begin{align}
\operatorname{AIC} &amp;= 2k-2l(\hat{\theta}|\mathbf{y}).
\end{align}
\]</span> The AIC is a relative measure, not absolute and is only useful in comparing two (or more) competing models. There is no guarantee that the best model (the one with the smallest AIC) of a set of models is the overall best model (there may be other unknown or untested models that are better).</p>
</div>
<div id="section-bayesian-information-criteria" class="section level3">
<h3>Bayesian Information Criteria</h3>
<p style="margin-bottom:0.3cm;">
</p>
<p>The Bayesian Information Criteria (BIC) is similar to the AIC, differing on the surface only in how model complexity in penalised.</p>
<p>Given a sample <span class="math inline">\(\mathbf{y}=y_1,y_2,\ldots,y_n\)</span> the Bayesian Information Criteria is <span class="math display">\[
\operatorname{BIC}=n\log(k)-2l(\hat{\theta}|\mathbf{y})
\]</span></p>
<p>The BIC is used much the same way as the AIC for comparing two competing models, but it doesn’t have the same useful interpretation via the <span class="math inline">\(\chi^2\)</span> distribution as the AIC. Instead, there are some heuristic rules of thumb for interpreting the strength of evidence for differences in BIC.</p>
<div class="table">
<table>
<thead>
<tr class="header">
<th>Difference in BIC</th>
<th>Strength of Evidence Against Higher BIC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0 to 2</td>
<td>Not worth mention</td>
</tr>
<tr class="even">
<td>2 to 6</td>
<td>Positive</td>
</tr>
<tr class="odd">
<td>6 to 10</td>
<td>Strong</td>
</tr>
<tr class="even">
<td>&gt;10</td>
<td>Very Strong</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="section-theory-questions" class="section level2">
<h2>Theory questions</h2>
<div id="section-question-1" class="section level3 boxed">
<h3>Question 1</h3>
<p>Let <span class="math inline">\(x_{1},x_{2},\ldots,x_{n}\)</span> be data distributed according to an inverse exponential distribution with pdf <span class="math display">\[
f(x~\vert~\lambda) = \frac{\exp\{-(\lambda x)^{-1}\}}{\lambda x^2} \quad \text{for} \quad x &gt; 0
\]</span></p>
<p>Note that <span class="math inline">\(\lambda &gt; 0\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Find the MLE of <span class="math inline">\(\lambda\)</span> for the inverse exponential distribution. What are the score functions for this distribution?</li>
<li>Show that <span class="math inline">\(T(\boldsymbol{x})= \sum_{i=1}^{n}x_{i}^{-1}\)</span> is the sufficient statistic for the distribution.</li>
<li>Write the deviance for the inverse exponential distribution in terms of the MLE, <span class="math inline">\(\hat{\lambda}\)</span>. What are the AIC and BIC formulae?</li>
<li>What is the expected<a href="#section-fn4" class="footnote-ref" id="section-fnref4"><sup>4</sup></a> Fisher information, <span class="math inline">\(\mathcal{I}_{n}(\lambda)\)</span>, and observed information, <span class="math inline">\(\mathcal{J}_{n}(\lambda)\)</span>, for the inverse exponential distributed data? Determine also the observed information, evaluated at the MLE.</li>
<li>Is the inverse exponential distribution in the exponential family? Why or why not?</li>
</ol>
</div>
<p><button id="solution_button" type="button" class="btn btn-default action-button" onclick="button_handler1()">Solution</button></p>
<div id="tq1-solution" style="display:none"><p><em>Solution</em>: </p>

<p>a. The log-likelihood is
\[
l(\lambda) = -n \log \lambda - 2\sum_{i=1}^{n}\log x_{i} - \frac{1}{\lambda} \sum_{i=1}^{n} x_{i}^{-1}
\]
The first partial derivative (or score function) is
\[
\frac{\partial l(\lambda)}{\partial \lambda} = -\frac{n}{\lambda} +  \frac{1}{\lambda^2} \sum_{i=1}^{n} x_{i}^{-1}
\]
which setting to zero, finds the MLE:
\[
\hat{\lambda} =  \frac{\sum_{i=1}^{n} x_{i}^{-1}}{n}
\]
b. The likelihood is </p>

<p>\[
L(\lambda) = \exp\left(-\lambda \sum_{i=1}^{n}x_{i}^{-1}\right) \lambda^{-n} \left(\prod_{i=1}^{n}x_{i}^2\right)^{-1}
\]
which can be written as 
\[
L(\lambda) = h(\boldsymbol{x}) g(\lambda,T(\boldsymbol{x}))
\]
where 
\[
h(\boldsymbol{x}) = \left(\prod_{i=1}^{n}x_{i}^2\right)^{-1}
\]</p>

<p>\[
g(\lambda,T(\boldsymbol{x}))= \exp\left(-\lambda T(\boldsymbol{x}) \right) \lambda^{-n}
\]</p>

<p>\[
T(\boldsymbol{x}) = \sum_{i=1}^{n}x_{i}^{-1}.
\]</p>

<p>Therefore by the Fisher–Neyman factorisation theorem, \(T(\boldsymbol{x})\) is sufficient.</p>

<p>c. The deviance is comprised of the log-likelihood at the MLE
\[
l(\hat\lambda~\vert~\boldsymbol{x}) = -n \log \hat\lambda - 2\sum_{i=1}^{n}\log x_{i} - \frac{1}{\hat\lambda} \sum_{i=1}^{n} x_{i}^{-1}
\]
and the log-likelihood for the saturated model. The saturated model occurs when \(\lambda\) is unique for each observation. The deviance from one observation is
\[
-\log \lambda_{i} - 2\log x_{i} - \frac{1}{\lambda_{i}}  x_{i}^{-1}
\]
for which the best \(\lambda_{i}\) is \(\hat{\lambda}_{i} = x_{i}^{-1}\), or in other words &ndash; the MLE for only one observation, hence the saturated model log-likelihood is
\[
l_{S}(\hat{\boldsymbol{\lambda}}_S~\vert~\boldsymbol{x}) = -\sum_{i=1}^{n}\left( \log \hat{\lambda}_{i} + 2\log x_{i} + \frac{1}{\hat{\lambda}_{i}}  x_{i}^{-1}\right) 
\]
\[
= -\sum_{i=1}^{n}\left( \log x_{i}^{-1} + 2\log x_{i} + 1\right)
\]
\[
= -\sum_{i=1}^{n}\log x_{i} - n
\]
hence the deviance is:
\[
D = 2\left(l_{S}(\hat{\boldsymbol{\lambda}}_S~\vert~\boldsymbol{x}) - l(\hat{\boldsymbol{\lambda}}~\vert~\boldsymbol{x})\right)
\]
\[
= 2 \left(-\sum_{i=1}^{n}\log x_{i} - n + (n \log \hat\lambda + 2\sum_{i=1}^{n}\log x_{i} + \frac{1}{\hat\lambda} \sum_{i=1}^{n} x_{i}^{-1}) \right) 
\]
\[
= 2 \left(\sum_{i=1}^{n}\log x_{i} + n (\log \hat\lambda -1) + \frac{1}{\hat\lambda} \sum_{i=1}^{n} x_{i}^{-1} \right)
\]</p>

<p>The AIC and BIC are</p>

<p>\[
\text{AIC} = 2(1) - 2l(\hat\lambda~\vert~\boldsymbol{x}) = 2
\]
\[
\text{BIC} = \log(n)(1) - 2l(\hat\lambda~\vert~\boldsymbol{x})
\]</p>

<p>respectively.</p>

<p>d. The second derivative of the log-likelihood is
\[
\frac{\partial^2 l(\lambda)}{\partial \lambda^2} = \frac{n}{\lambda^2}  -\frac{2}{\lambda^3} \sum_{i=1}^{n} x_{i}^{-1}
\]
The expected value \(E(x^{-1}) = \lambda\) and so the expected information is
\[
\mathcal{I}_{n}(\lambda) = -E\left(\frac{\partial^2 l(\lambda)}{\partial \lambda^2}\right) = \frac{n}{\lambda^2}  -\frac{2}{\lambda^3} n\lambda = \frac{n}{\lambda^2}
\]</p>

<p>The observed information is</p>

<p>\[
\mathcal{J}_{n}(\lambda) = \frac{n}{\lambda^2}  -\frac{2}{\lambda^3} \sum_{i=1}^{n} x_{i}^{-1}
\]
and if evaluated at the MLE we have:</p>

<p>\[
\mathcal{J}_{n}(\hat{\lambda}) = \frac{n}{\hat{\lambda}^2}
\]
e. Yes, it is in the exponential family:</p>

<p>The likelihood is
\[
L(\lambda) = \exp\left(-\lambda \sum_{i=1}^{n}x_{i}^{-1}\right) \lambda^{-n} \left(\prod_{i=1}^{n}x_{i}^2\right)^{-1}
\]
which we compare to the exponential family form
\[
h(\boldsymbol{x}) \exp\left[\eta(\lambda) \cdot T(\boldsymbol{x}) - A(\lambda)\right]
\]
to see that the likelihood has this form with</p>

<ul>
<li>\(h(\boldsymbol{x})  = \left(\prod_{i=1}^{n}x_{i}^2\right)^{-1}\)</li>
<li>\(\eta(\lambda) = -\lambda\)</li>
<li>\(T(\boldsymbol{x}) = \sum_{i=1}^{n}x_{i}^{-1}\)</li>
<li>\(A(\lambda) = n \log \lambda\)</li>
</ul>
</div>
<script type="text/javascript">
function button_handler1() {
  var x = document.getElementById('tq1-solution');
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}
</script>
<div id="section-question-2" class="section level3 boxed">
<h3>Question 2</h3>
<p>Let <span class="math inline">\(x_{1},x_{2},\ldots,x_{n}\)</span> be data distributed according to a Weibull distribution with pdf <span class="math display">\[
g(x~\vert~\lambda, \kappa) = \frac{\kappa}{\lambda}\left(\frac{x}{\lambda}\right)^{\kappa-1}\exp\{-(x/\lambda)^{\kappa}\} \quad \text{for} \quad x &gt; 0
\]</span> Note that <span class="math inline">\(\lambda &gt; 0\)</span> and <span class="math inline">\(\kappa &gt; 0\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Write down the equations which determine the MLE for the Weibull distributed data. Find the MLE of <span class="math inline">\(\lambda\)</span> in terms of <span class="math inline">\(\kappa\)</span>.</li>
<li>What are the AIC and BIC formulae for the Weibull distribution?</li>
<li>For <span class="math inline">\(n=20\)</span> does the AIC or BIC penalise larger models more?</li>
<li>Is the Weibull distribution in the exponential family? Why or why not?</li>
</ol>
</div>
<p><button id="solution_button" type="button" class="btn btn-default action-button" onclick="button_handler2()">Solution</button></p>
<div id="tq2-solution" style="display:none"><p>a. The log-likelihood is
\[
l(\lambda,k) = n\log k  + (k-1)\sum_{i=1}^{n}\log(x_{i}) - kn \log \lambda  -\sum_{i=1}^{n}\left(\frac{x_{i}}{\lambda}\right)^{k}
\]
Setting the score functions (first derivatives) to zero we need to solve:
\[
\frac{\partial l}{\partial \lambda} = -\frac{kn}{\lambda} + k \lambda^{-k-1}\sum_{i=1}^{n}x_{i}^{k} = 0
\]
\[
\frac{\partial l}{\partial k} = \frac{n}{k} + \sum_{i=1}^{n}\log(x_{i}) - n \log \lambda - \sum_{i=1}^{n}\log \left( \frac{x_{i}}{k} \right) \left(\frac{x_{i}}{\lambda}\right)^{k} = 0
\]</p>

<p>The first equations simplifies to \(\hat{\lambda}(k) = \left(\frac{\sum_{i=1}^{n}x_{i}^{k}}{n}\right)^{1/k}\).</p>

<p>b. The unnormalised deviance (i.e. no saturated model) is</p>

<p>\[
\text{dev} = -2n\log k  -2 (k-1)\sum_{i=1}^{n}\log(x_{i}) +2 kn \log \lambda  +2\sum_{i=1}^{n}\left(\frac{x_{i}}{\lambda}\right)^{k}
\]
\(\text{AIC} = 2(2) + \text{dev} = 4 + \text{dev}\)</p>

<p>\(\text{BIC} = (\log n) (2) + \text{dev} = (\log n) (2) + \text{dev}\)</p>

<p>c. For \(n = 20\) \(\log(20) \geq 2\) therefore BIC penalises the number of parameters more. For the current example:</p>

<p>\(\text{AIC} = 4 + \text{dev}\) and \(\text{BIC} \approx 6 + \text{dev}\).</p>

<p>d. No, it is not in the exponential family as the parameter \(k\) and data \(x_{i}\) do not interact linearly in the exponential function. That is, \(\sum_{i=1}^{n}\left(\frac{x_{i}}{\lambda}\right)^{k}\) cannot be written linearly in \(k\).</p>
</div>
<script type="text/javascript">
function button_handler2() {
  var x = document.getElementById('tq2-solution');
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}
</script>
<div id="section-question-3" class="section level3 boxed">
<h3>Question 3</h3>
<p>For normally distributed data, <span class="math inline">\(x_{1},x_{2},\ldots,x_{n}\)</span> show that distribution is in the exponential family. What are the sufficient statistics of the normal distribution?</p>
</div>
<p><button id="solution_button" type="button" class="btn btn-default action-button" onclick="button_handler3()">Solution</button></p>
<div id="tq3-solution" style="display:none"><p><em>Solution</em>: None provided &ndash; self check.</p>

<p>The likelihood is
\[
L(\mu,\sigma^2|\mathbf{x})=\left(\frac{1}{2\pi\sigma^2}\right)^{n/2}\exp\left(-\frac{\sum_{i=1}^n(x_i-\mu)^2}{2\sigma^2}\right)
\]
and the log-likelihood is 
\[
l(\mu,\sigma^2|\mathbf{x})=-\frac{n}{2}\log\left(2\pi\sigma^2\right)-\frac{\sum_{i=1}^n(x_i-\mu)^2}{2\sigma^2}.
\]</p>
</div>
<script type="text/javascript">
function button_handler3() {
  var x = document.getElementById('tq3-solution');
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}
</script>
</div>
<div id="section-practical-questions" class="section level2">
<h2>Practical questions</h2>
<div id="section-question-4" class="section level3 boxed">
<h3>Question 4</h3>
<ol style="list-style-type: decimal">
<li>In this question we will compare how the AIC and BIC perform under simulated data. Assume the true data is generated from a Weibull distribution with <span class="math inline">\(\lambda = 2\)</span> and <span class="math inline">\(\kappa = 1\)</span>. This can easily be simulated in <code>R</code><a href="#section-fn5" class="footnote-ref" id="section-fnref5"><sup>5</sup></a>. Start by considering the case where <span class="math inline">\(n=20\)</span>.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Write two functions to find the MLEs of the inverse exponential and Weibull distribution<a href="#section-fn6" class="footnote-ref" id="section-fnref6"><sup>6</sup></a>.</li>
<li>Write functions to calculate the AIC and BIC for given parameter values of the inverse exponential and Weibull distributions.</li>
<li>Using the code in the answer template, complete the function that performs one simulation of the information criteria decision we are trying to evaluate.</li>
<li>Run the simulation under a variety of settings (e.g. small <span class="math inline">\(n\)</span> versus large <span class="math inline">\(n\)</span>) and comment on how accurate the information criteria are under each of these scenarios.</li>
</ol>
</div>
<p><em>Solution</em>:</p>
<pre class="r"><code>  # (a) Write function to find log-likelihoods and MLEs:</code></pre>
<pre class="r"><code>  # (a) Write function to find log-likelihoods and MLEs:

  log_lhood_invexp &lt;- function(params, x){
    # params[1] lambda / scale
    
    n &lt;- length(x)
    
    sum_log_x &lt;- sum(log(x))
    sum_inv_x &lt;- sum(1/x)
    
    ll &lt;- -n * log(params[1]) - 2 * sum_log_x -  sum_inv_x / params[1]
    
    return(ll)
    
  }
  
  invexp_mle &lt;- function(x){
    
    n &lt;- length(x)
    
    mle &lt;- sum(1/x) / n
    
    return(mle)
    
  }</code></pre>
<pre class="r"><code>  log_lhood_weibull &lt;- function(params, x){
    # params[1] lambda / scale
    # params[2] kappa / shape
    
    n &lt;- length(x)
    
    ll &lt;- sum(
      dweibull(x = x, 
               scale = params[1], 
               shape = params[2], 
               log = T)
      )
    
    return(ll)
    
  }

  weibull_mle &lt;- function(x){
    
    start_par &lt;- c(0.1,0.1)
    
    # complete this line
    optim_res &lt;- optim(par = start_par, 
                       fn = log_lhood_weibull, 
                       x = x,
                       control = list(fnscale = -1)
                       )
    # in this case I didn&#39;t add the constraints lambda, kappa &gt; 0
    # because it dweibull(...) automatically return NaN for invalid
    # parameter values.
    
    return(optim_res$par)
    
  }
  
  # make sure you test the functions.</code></pre>
<pre class="r"><code>  # (b) Write function to find AIC and BIC:</code></pre>
<pre class="r"><code>  aic_invexp &lt;- function(params, x){
    
    2 * ( length(params) - log_lhood_invexp(params = params, x = x)) 
    
  }

  bic_invexp &lt;- function(params, x){
    
    n &lt;- length(x)
    
    log(n) * length(params) - 
       2 * log_lhood_invexp(params = params, x = x)
    
  }
  
  aic_weibull &lt;- function(params, x){
    
     2 * ( length(params) - log_lhood_weibull(params = params, x = x)) 
    
  }

  bic_weibull &lt;- function(params, x){
    
    n &lt;- length(x)
    
    log(n) * length(params) - 
       2 * log_lhood_weibull(params = params, x = x)
    
  }
  
  # make sure you test the functions.</code></pre>
<pre class="r"><code>  sim_weibull_vs_invexp &lt;- function(n_obs, true_lambda, true_kappa){
    
    # simulate data
    sim_x &lt;- rweibull(n = n_obs, scale = true_lambda, shape = true_kappa)
    
    # fit MLEs for each model
    invexp_est &lt;- invexp_mle(x = sim_x)
    weibull_est &lt;- weibull_mle(x = sim_x)
    
    
    # compute AIC/BIC
    aic_iexp &lt;- aic_invexp(params = invexp_est, x = sim_x)
    bic_iexp &lt;- bic_invexp(params = invexp_est, x = sim_x)
    aic_wei &lt;- aic_weibull(params = weibull_est, x = sim_x)
    bic_wei &lt;- bic_weibull(params = weibull_est, x = sim_x)

      
    # Check if correct decision made using AIC
    correct_aic &lt;- aic_iexp &gt; aic_wei
    
    # Check if correct decision made using BIC
    correct_bic &lt;- bic_iexp &gt; bic_wei
    
    # save and return results
    out_tb &lt;- tibble(
      correct_aic = correct_aic,
      correct_bic = correct_bic,
      sqerror_weibull_lambda = (weibull_est[1] - true_lambda)^2,
      sqerror_weibull_kappa = (weibull_est[2] - true_kappa)^2,
      sqerror_invexp_lambda = (invexp_est - true_lambda)^2
    )
    
    return(out_tb)
    
  }

  # make sure you test the functions.</code></pre>
<pre class="r"><code>  sim_results &lt;- rerun(.n = 1000, sim_weibull_vs_invexp(n_obs = 20, 
                        true_lambda = 2, true_kappa = 1)) %&gt;%
                          bind_rows()

  # inspect results
  sim_results %&gt;% 
    summarise(mean(correct_aic),mean(correct_bic))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["mean(correct_aic)"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["mean(correct_bic)"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.889","2":"0.861"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>  sim_results %&gt;% 
    summarise(mean(sqerror_weibull_lambda), 
              mean(sqerror_weibull_kappa),
              mean(sqerror_invexp_lambda))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["mean(sqerror_weibull_lambda)"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["mean(sqerror_weibull_kappa)"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["mean(sqerror_invexp_lambda)"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.2412126","2":"0.046592","3":"400.9264"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["header-attrs"]},{"type":"character","attributes":{},"value":["2.7"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pandoc"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["header-attrs.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/yeti.min.css"]},{"type":"character","attributes":{},"value":["<style>h1 {font-size: 34px;}\n       h1.title {font-size: 38px;}\n       h2 {font-size: 30px;}\n       h3 {font-size: 24px;}\n       h4 {font-size: 18px;}\n       h5 {font-size: 16px;}\n       h6 {font-size: 12px;}\n       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}\n       pre:not([class]) { background-color: white }<\/style>"]},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["pagedtable"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pagedtable-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/pagedtable.js"]},{"type":"character","attributes":{},"value":["css/pagedtable.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["textmate.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-format"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmarkdown/templates/tutorial/resources"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-format.js"]},{"type":"character","attributes":{},"value":["tutorial-format.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["navigation"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/navigation-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tabsets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["default.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]}]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94]}},"value":[{"type":"character","attributes":{},"value":["assertthat","backports","base","broom","bslib","cellranger","cli","colorspace","compiler","crayon","datasets","DBI","dbplyr","digest","dplyr","ellipsis","evaluate","fansi","farver","fastmap","forcats","fs","generics","ggforce","ggplot2","ggtext","glue","graphics","grDevices","grid","gridExtra","gridtext","gtable","haven","hms","htmltools","htmlwidgets","httpuv","httr","jquerylib","jsonlite","kableExtra","knitr","later","learnr","lifecycle","lubridate","magrittr","markdown","MASS","methods","mime","modelr","munsell","MXB341","pander","pillar","pkgconfig","polyclip","pracma","promises","purrr","R6","Rcpp","readr","readxl","reprex","rlang","rmarkdown","rprojroot","rstudioapi","rvest","sass","scales","shiny","stats","stringi","stringr","tibble","tidyr","tidyselect","tidyverse","tools","tweenr","utf8","utils","vctrs","viridisLite","webshot","withr","xfun","xml2","xtable","yaml"]},{"type":"character","attributes":{},"value":["0.2.1","1.2.1","4.0.2","0.7.5","0.2.4.9001","1.1.0","2.3.1","2.0-0","4.0.2","1.4.1","4.0.2","1.1.1","2.1.0","0.6.27","1.0.4","0.3.1","0.14","0.4.2","2.0.3","1.1.0","0.5.1","1.5.0","0.1.0","0.3.2","3.3.3","0.1.1","1.4.2","4.0.2","4.0.2","4.0.2","2.3","0.1.4","0.3.0","2.3.1","1.0.0","0.5.1.1","1.5.3","1.5.5","1.4.2","0.1.3","1.7.2","1.3.1","1.31","1.1.0.1","0.10.1","1.0.0","1.7.9.2","2.0.1","1.1","7.3-53.1","4.0.2","0.10","0.1.8","0.5.0","1.0.0.2021","0.6.3","1.5.0","2.0.3","1.10-0","2.3.3","1.2.0.1","0.3.4","2.5.0","1.0.6","1.4.0","1.3.1","1.0.0","0.4.10","2.7","2.0.2","0.13","0.3.6","0.3.1","1.1.1","1.6.0","4.0.2","1.5.3","1.4.0","3.0.6","1.1.2","1.1.0","1.3.0","4.0.2","1.0.1","1.1.4","4.0.2","0.3.6","0.3.0","0.5.2","2.4.1","0.21","1.3.2","1.8-4","2.2.1"]}]}]}
</script>
<!--/html_preserve-->
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="section-fn1"><p>“The future of data analysis”. Annals of Mathematical Statistics 33(1), 1962, pg 13.<a href="#section-fnref1" class="footnote-back">↩︎</a></p></li>
<li id="section-fn2"><p>This applies as well to the the discrete case, i.e. probability mass functions.<a href="#section-fnref2" class="footnote-back">↩︎</a></p></li>
<li id="section-fn3"><p>Note that the MLE is often used to calculate a numerical value for the expected information using <span class="math inline">\(\mathcal{I}_{n}\left(\hat{\boldsymbol{\theta}}\right)\)</span>.<a href="#section-fnref3" class="footnote-back">↩︎</a></p></li>
<li id="section-fn4"><p>Hint: The expected value of <span class="math inline">\(x^{-1}\)</span> from an inverse exponential distribution (the given parametrisation) is <span class="math inline">\(\lambda\)</span>.<a href="#section-fnref4" class="footnote-back">↩︎</a></p></li>
<li id="section-fn5"><p>Use the code <code>rweibull(n = 20, shape = 1, scale = 2)</code> for a Weibull with scale <span class="math inline">\(\lambda = 2\)</span> and shape <span class="math inline">\(\kappa=1\)</span>.<a href="#section-fnref5" class="footnote-back">↩︎</a></p></li>
<li id="section-fn6"><p>The Weibull will require a numerical solver such as <code>optim()</code>.<a href="#section-fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

</div> <!-- topics -->

<div class="topicsContainer">
<div class="topicsPositioner">
<div class="band">
<div class="bandContent topicsListContainer">

<!-- begin doc-metadata -->
<div id="doc-metadata">
<h2 class="title toc-ignore" style="display:none;">MXB341 Worksheet 3 - Maximum Likelihood Estimators</h2>
</div>
<!-- end doc-metadata -->

</div> <!-- bandContent.topicsListContainer -->
</div> <!-- band -->
</div> <!-- topicsPositioner -->
</div> <!-- topicsContainer -->


</div> <!-- bandContent page -->
</div> <!-- pageContent band -->




<script>
// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


</body>

</html>
