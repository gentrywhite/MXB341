<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title></title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>



<!-- MathJax scripts -->
<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<hr/>

<p><em>Solution</em>: </p>

<p>(a) If \(y_{i} \sim \text{Bin}(k,\theta)\), then \(s = \sum_{i=1}^n y_i \sim \text{Bin}(nk,\theta)\). Proof omitted, using MGF is one option.</p>

<p>(b) \[\begin{aligned}
        \mathcal{I}_{n}(\theta) &= E\left[- \frac{d^2 \ell(\theta)}{d \theta^2}\right]\
        &= E\left[\sum_{i=1}^n \frac{y_i}{\theta^2} - \frac{k-y_i}{(1-\theta)^2} \right]\
        &= \sum_{i=1}^n \left[\frac{E(y_i)}{\theta^2}- \frac{k-E(y_i)}{(1-\theta)^2} \right]\
        &= \sum_{i=1}^n \left[\frac{k}{\theta}- \frac{k}{1-\theta} \right] \qquad \text{by noting } E(y_i)=k\theta\
        &= \frac{nk}{\theta(1-\theta)}.
    \end{aligned}\] 
thus the CRB is: \(\text{var}(\hat{\theta}) \geq \frac{\theta(1-\theta)}{nk}\).</p>

<p>&copy; The log-likelihood is \(\ell(\theta) = \sum_{i=1}^{n} \left\{ y_{i} \log \theta + (k-y_i)\log (1-\theta) \right\} + C.\) The first and second derivatives are
    \[\begin{aligned}
        \frac{d \ell(\theta)}{d \theta} &= \sum_{i=1}^n \frac{y_i}{\theta} - \frac{k-y_i}{1-\theta}\
        \frac{d^2 \ell(\theta)}{d \theta^2} &= -\sum_{i=1}^n \frac{y_i}{\theta^2} - \frac{k-y_i}{(1-\theta)^2}.
    \end{aligned}\]</p>

<p>The MLE \(\hat{\theta}\) is found by solving
    \[\begin{aligned}
        \frac{d \ell(\theta)}{d \theta}|_{\theta=\hat{\theta}} &= \sum_{i=1}^n \frac{y_i}{\hat{\theta}} - \frac{k-y_i}{1-\hat{\theta}}=0\
        \text{which gives } \hat{\theta} &= \frac{\sum_{i=1}^ny_i}{nk},
    \end{aligned}\]
    the pooled proportion of successes. Note that \(-\frac{d^2 \ell(\theta)}{d \theta^2}\) is \(>0\) for all \(\theta\in (0,1)\) (a convex optimisation) so that the MLE is a maximum and unique.</p>

<p>The expectation of the MLE is given by
    \[\begin{aligned}
        E(\hat{\theta}) &= E\left(\frac{\sum_{i=1}^n y_i}{nk}\right)\
        &= \sum_{i=1}^n\frac{E\left(y_i\right)}{nk}\
        &= \sum_{i=1}^n \frac{\theta}{n}, \quad \text{by noting } E(y_i)=k\theta\
        &=\theta,
    \end{aligned}\]
    and hence the maximum likelihood estimate \(\hat{\theta}\) is an unbiased estimate of \(\theta\).</p>

<p>(d) The variance of the MLE is given by
        \[\begin{aligned}
        \text{var}(\hat{\theta}) &= \text{var}\left(\frac{1}{nk}\sum_{i=1}^n y_i\right)\
        &=\frac{1}{n^2k^2}\text{var}\left(\sum_{i=1}^n y_i\right)\
      &=\frac{1}{n^2k^2}\text{var}(s)\
        &=\frac{nk\theta(1-\theta)}{n^2k^2} \quad \text{using part (a)} \
        &= \frac{\theta(1-\theta)}{nk}.
    \end{aligned}\] Hence we note that the MLE \(\hat{\theta}\) is an <strong>efficient estimator</strong> since its variance is equal to the Cram√©r-Rao minimum variance bound.</p>

<hr/>

</body>

</html>
