---
title: "Week 12"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Week 12}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(gganimate)
library(animation)
```

```{r, echo=FALSE}

htmltools::includeHTML("header.html")
htmltools::includeCSS("QUTReadings.css")

```

```

#   MCMC Diagnostic Tools: What went wrong? 

Previously, we have seen how to use Monte Carlo integration and algorithms like
the Metropolis-Hastings algorithm to evaluate and make inference on a
wide variety of Bayesian models. It is important to note Monte Carlo
methods are an approximation to an exact solution, the approximation is
asymptotically accurate, but there is some error, as there is in any
sample-based estimate. The error reduces as the sample size increases.
Still, the accuracy of the estimates relies on the assumption of
independent samples drawn from the target distribution, assumptions
which should be verified as much as possible. We have several tools available for us to examine the results of our sampling schemes and verify if the results are actually from the target density, and assess the quality of our samples for making inference. 

Previously we have seen how to evaluate Bayesian models with
non-conjugate priors in the univariate case, but in practice, most
interesting and useful models have multiple parameters and may involve
complex hierarchical structures. With the Metropolis-Hastings algorithm,
the only option is to draw samples jointly from the target posterior.
Joint sampling with the Metropolis-Hastings algorithm can be difficult
as the acceptance probability naturally decreases as the dimension of
the parameter vector being sampled increases. There are two methods
available to address this, the Hit-and-Run Sampler which in some cases
can reduce a multivariate sampling situation to a univariate one. The
second is the Gibbs sampling scheme, which can accommodate both
multivariate and hierarchical models. In this Section, we will also
explore these algorithms and schemes for evaluating more complex models.

##    Convergence Diagnostics and Burn-in: Testing Staionarity 

When constructing a Markov Chain Monte Carlo (MCMC) scheme, initial
values are selected for the parameters and, in the case of the
Metropolis-Hastings Algorithm (and others) initial values for the
candidate distribution. These initial values are equivalent to choosing
a starting point for exploring the target density. It is quite possible,
and in some cases desirable, to choose initial values that are extreme
or in areas of low posterior density. A properly constructed MCMC scheme
will converge to stationarity, generating samples from the target
posterior. It is good practice; therefore, to monitor the Markov chain
for convergence, identify when the convergence has occurred and discard
the initial samples to ensure that the remaining samples are from a stationary
ergodic process. This referred to as "burn-in" and can be monitored both
graphically and analytically.

### Cumulative Mean Plots for Graphically Monitoring Convergence

```{r echo=FALSE}

set.seed(14061972)
x<-rnorm(15,10,1)
mu<-4.7*x-1.3
y<-rnbinom(15,mu=mu,size=1.6)

```

:::{.sidenote}
**Example:**
Consider a sample 
$\mathbf{y}=y_1,\ldots,y_n$ from $Y\sim Neg-Binom(\boldsymbol{\mu},\eta)$ where 
$\log(\mu_i)=\beta_0+\beta_1x_i$ for some covariate $x_i$. 

For $y=$ `r y` and $x=$ `r round(x,2)` fit the generlised linear model assuming the data are from a negative-binomial distribution using the priors
\begin{eqnarray}
\pi(\boldsymbol{\beta})&\propto& 1\\
\pi(\eta)&=&e^{-\eta}
\end{eqnarray}

Using the naive, improved, and Hessian candidates, draw $10,\!000$ samples from the posteriors for $\beta_0$ and $\beta_1$ and plot the cummulative means. 

:::

The trace plot i.e. the plot of all values generated by an MCMC scheme
plotted sequentially is the first and most useful tool for examining the
behaviour of the MCMC scheme. The trace plot can show extreme outliers
and possible multi-modality, but it provides little information about
the stationarity of the process.

For a target variable $\theta$ and an MCMC scheme generating a sequence
of samples $\theta^{(t)},\theta^{(t+1)},\ldots$ from the target
posterior $\pi(\theta|\mathbf{y})$ the most basic definition of stationarity
is
$$
E\left(\theta^{(t)}|\mathbf{y}\right)=E\left(\theta^{(t+1)}|\mathbf{y}\right).
$$
From this definition it is reasonable to assume that monitoring the
cumulative average of the samples generated by the MCMC scheme can be
used as a tool for monitoring convergence, by plotting $t$ versus the
cumulative mean, i.e. for $t=1,\ldots,T$ plot $t$ versus
$$
\bar{\theta}^{(t)}=\frac{1}{t}\sum_{s=1}^t\theta^{(s)}
$$ 
The resulting
plot should reach an stable value for $\bar{\theta}^{(t)}$, indicating
the point where the MCMC scheme achieves stationarity.

:::{.boxed}

### Example {.tabset .tabset-pills}

Consider a sample 
$\mathbf{y}=y_1,\ldots,y_n$ from $Y\sim Neg-Binom(\boldsymbol{\mu},\eta)$ where 
$\log(\mu_i)=\beta_0+\beta_1x_i$ for some covariate $x_i$. 

For $y=$ `r y` and $x=$ `r round(x,2)` fit the generlised linear model assuming the data are from a negative-binomial distribution using the priors
\begin{eqnarray}
\pi(\boldsymbol{\beta})&\propto& 1\\
\pi(\eta)&=&e^{-\eta}
\end{eqnarray}

Using the naive, improved, and Hessian candidates, draw $10,\!000$ samples from the posteriors for $\beta_0$ and $\beta_1$ and plot the cummulative means. 

####    Solution

The scatter plot of the data 
```{r echo=FALSE,message=FALSE}

df<-tibble(x,y)
ggplot(df,aes(x=x,y=y))+
  geom_point()+
  geom_smooth(method=lm,se = TRUE)
```
reveals that there does appear to be *some* linear relationshiop between $x$ and $y$.  But the $y$ values are from a neagtive binomial distribution so we will need to fit a GLM to make inferences about the relationship between $x$ and $y$. 

The likelihood is 
$$
f(\mathbf{y}|\boldsymbol{\beta},\eta)=\prod_{i=1}^n\frac{\Gamma(y_i+\eta)}{\Gamma(\eta)y_i!}\left(\frac{\eta}{\eta+\exp(\beta_0+\beta_1x_i)}\right)^\eta\left(\frac{\exp(\beta_0+\beta_1x_i)}{\eta+\exp(\beta_0+\beta_1x_i)}\right)^{y_i}
$$
Which is quite messy, analytical solutions to find the maximum likelihood estimators for $\beta_0$, $\beta_1$, and $\sigma$ are not easily (if at all) available.  There are also no conjugate priors available either. The resulting full conditionals though are
\begin{eqnarray}
\pi(\boldsymbol{\beta}|\mathbf{y},\sigma)&\propto&\prod_{i=1}^n\frac{\Gamma(y_i+\eta)}{\Gamma(\eta)y_i!}\left(\frac{\eta}{\eta+\exp(\beta_0+\beta_1x_i)}\right)^\eta\left(\frac{\exp(\beta_0+\beta_1x_i)}{\eta+\exp(\beta_0+\beta_1x_i)}\right)^{y_i}\\
\pi(\sigma|\mathbf{y},\boldsymbol{\beta})&\propto&\prod_{i=1}^n\frac{\Gamma(y_i+\eta)}{\Gamma(\eta)y_i!}\left(\frac{\eta}{\eta+\exp(\beta_0+\beta_1x_i)}\right)^\eta\left(\frac{\exp(\beta_0+\beta_1x_i)}{\eta+\exp(\beta_0+\beta_1x_i)}\right)^{y_i}e^{-\sigma}
\end{eqnarray}
Sampling from either of these will require using a Metropolis-Hastings step or a slice sampling step.  For a Metropolis-Hastings step we will need a good candidate distribution.  Because $(\beta_0,\beta_1)\in\mathbb{R}^2$ a natural choice of candidate is a bivariate Gaussian distribution (which has the added benefit of being symmetric, which simplifies computing the acceptance probability).  A naive choice for a random walk candidate distribution would be
$$
\boldsymbol{\beta}^c|\boldsymbol{\beta}_{t-1}\sim N(\boldsymbol{\beta}_{t-1},\delta\mathbf{I})
$$
where $\delta$ is some value chosen to obtain a desirable acceptance rate. 
Alternatively, we could use a bivariate Gaussian candidate with a covariance $a\mathbf{H}$ where $\mathbf{H}$ is the Hessian of the likelihood function and $a$ is some constant chose to obtain a desirable acceptance rate. Indeed, we could use an independent Metropolis-Hastings step by then centring the candidate distribution at the mode of the likelihood.  For high dimensional problems obtaining the Hessian, it can be approximated as $\hat{\mathbf{H}}=\left(\mathbf{X}^T\mathbf{X}\right)$ where $\mathbf{X}$ is the designe matrix of the linear regression.  

The candidate for the parameter $\sigma$ may be trickier.  In this case it is useful to note that the Gamma distribution can be parameterised in terms of its mean and variance
$$
\pi(\eta)=\frac{\left(\frac{\mu}{\sigma^2}\right)^{\frac{\mu^2}{\sigma^2}}}{\Gamma\left(\frac{\mu^2}{\sigma^2}\right)}\eta^{\frac{\mu^2}{\sigma^2}-1}\exp\left(-\eta\frac{\mu}{\sigma^2}\right)
$$
The the candidate distribution can be constructed with the mean of $\eta_{t-1}$ and the variance chosen to obtain the desired acceptance rate.  

The cummulative mean at iteration $i$ is 
$$
\bar{y}_i=\frac{\sum_{j=1}^iy_j}{i}.
$$
We can compute this easily in `R` and plot the results  

####    Code

We will look at three sets of code and the results to see the effects of the various candidate distributions.

Example 1: Using the naive candidate

```{r}

library(mvtnorm)

set.seed(14061972) 

X<-cbind(1,x)

log_like<-function(BETA,ETA)
{
  mu<-X%*%BETA%>%exp()
  dnbinom(y,size = ETA,mu = mu, log = TRUE)%>%sum()
}

m<-10000

beta<-matrix(NA,ncol=2,nrow=m)
eta<-numeric(length = m)

beta[1,]<-0
eta[1]<-1

q_sd<-0.01
beta_cov<-diag(q_sd,2)

eta_var<-2

for(i in 2:m)
{
  ##  update betas
  bc<-rmvnorm(1,mean = beta[i-1,],sigma = beta_cov)%>%t()
  alpha<-(log_like(bc,eta[i-1])-log_like(beta[i-1,],eta[i-1]))%>%exp()
  u<-runif(1)
  if(alpha>u)
  {
    beta[i,]<-bc
  }
  else
  {
    beta[i,]<-beta[i-1,]
  }
  
  ##  update eta
  ac<-eta[i-1]^2/eta_var
  bc<-eta[i-1]/eta_var
  
  ec<-rgamma(1,ac,bc)
  
  a<-ec^2/eta_var
  b<-ec/eta_var
  
  alpha<-(log_like(beta[i,],ec)-log_like(beta[i,],eta[i-1])+dgamma(eta[i-1],a,b,log=TRUE)-dgamma(ec,ac,bc,log = TRUE)-ec+eta[i-1])%>%exp()

u<-runif(1)
if(alpha>u)
{
  eta[i]<-ec
}
else
{
  eta[i]<-eta[i-1]
}

}

df_cummean_naive<-tibble(pars = c(cummean(beta[,1]),cummean(beta[,2]),cummean(eta)),names=c(rep("beta[0]",m),rep("beta[1]",m),rep("eta",m)),iteration = rep(1:m,3))

                        
beta_naive<-beta
b0_hat_naive<-colMeans(beta_naive)[1]%>%round(2)
b1_hat_naive<-colMeans(beta_naive)[2]%>%round(2)
eta_naive<-eta
eta_hat_naive<-mean(eta_naive)%>%round(2)

```


$$
\phantom{linebreak}
$$
Example 2: Using the improved candidate

```{r}

library(mvtnorm)

set.seed(14061972)

X<-cbind(1,x)

log_like<-function(BETA,ETA)
{
  mu<-X%*%BETA%>%exp()
  dnbinom(y,size = ETA,mu = mu, log = TRUE)%>%sum()
}

m<-10000

beta<-matrix(NA,ncol=2,nrow=m)
eta<-numeric(length = m)

beta[1,]<-0
eta[1]<-1

q_sd<-2
beta_cov<-q_sd*solve(t(X)%*%X)

eta_var<-2

for(i in 2:m)
{
  ##  update betas
  bc<-rmvnorm(1,mean = beta[i-1,],sigma = beta_cov)%>%t()
  alpha<-(log_like(bc,eta[i-1])-log_like(beta[i-1,],eta[i-1]))%>%exp()
  u<-runif(1)
  if(alpha>u)
  {
    beta[i,]<-bc
  }
  else
  {
    beta[i,]<-beta[i-1,]
  }

  ##  update eta
  ac<-eta[i-1]^2/eta_var
  bc<-eta[i-1]/eta_var

  ec<-rgamma(1,ac,bc)

  a<-ec^2/eta_var
  b<-ec/eta_var

  alpha<-(log_like(beta[i,],ec)-log_like(beta[i,],eta[i-1])+dgamma(eta[i-1],a,b,log=TRUE)-dgamma(ec,ac,bc,log = TRUE)-ec+eta[i-1])%>%exp()

u<-runif(1)
if(alpha>u)
{
  eta[i]<-ec
}
else
{
  eta[i]<-eta[i-1]
}

}

beta_improved<-beta
eta_improved<-eta
iteration<-1:m

df_cummean_improved<-tibble(pars = c(cummean(beta[,1]),cummean(beta[,2]),cummean(eta)),names=c(rep("beta[0]",m),rep("beta[1]",m),rep("eta",m)),iteration = rep(1:m,3))

```
$$
\phantom{linebreak}
$$

Example 3: Using the Hessian candidate

```{r}

library(mvtnorm)

set.seed(14061972)

X<-cbind(1,x)

log_like<-function(BETA,ETA)
{
  mu<-X%*%BETA%>%exp()
  dnbinom(y,size = ETA,mu = mu, log = TRUE)%>%sum()
}

ANS<-optim(c(0,0,1),function(x) -log_like(x[1:2],x[3]), lower = c(-100,-100,0), method = "L-BFGS-B", hessian = TRUE)

H<-solve(ANS$hessian[1:2,1:2])

m<-10000

beta<-matrix(NA,ncol=2,nrow=m)
eta<-numeric(length = m)

beta[1,]<-0
eta[1]<-1

q_sd<-3
beta_cov<-q_sd*H

eta_var<-2

for(i in 2:m)
{
  ##  update betas
  bc<-rmvnorm(1,mean = beta[i-1,],sigma = beta_cov)%>%t()
  alpha<-(log_like(bc,eta[i-1])-log_like(beta[i-1,],eta[i-1]))%>%exp()
  u<-runif(1)
  if(alpha>u)
  {
    beta[i,]<-bc
  }
  else
  {
    beta[i,]<-beta[i-1,]
  }

  ##  update eta
  ac<-eta[i-1]^2/eta_var
  bc<-eta[i-1]/eta_var

  ec<-rgamma(1,ac,bc)

  a<-ec^2/eta_var
  b<-ec/eta_var

  alpha<-(log_like(beta[i,],ec)-log_like(beta[i,],eta[i-1])+dgamma(eta[i-1],a,b,log=TRUE)-dgamma(ec,ac,bc,log = TRUE)-ec+eta[i-1])%>%exp()

u<-runif(1)
if(alpha>u)
{
  eta[i]<-ec
}
else
{
  eta[i]<-eta[i-1]
}

}

beta_hessian<-beta
eta_hessian<-eta

iteration<-1:m

df_cummean_hessian<-tibble(pars = c(cummean(beta[,1]),cummean(beta[,2]),cummean(eta)),names=c(rep("beta[0]",m),rep("beta[1]",m),rep("eta",m)),iteration = rep(1:m,3))

```

####    Plot

The GIFs below show the animation of the cummulative mean plots for the three parameter and the three different candidates. 

```{r echo=FALSE}

pars<-c(cummean(beta_naive[,1]),
        cummean(beta_improved[,1]),
        cummean(beta_hessian[,1]),
        cummean(beta_naive[,2]),
        cummean(beta_improved[,2]),
        cummean(beta_hessian[,2]),
        cummean(eta_naive),
        cummean(eta_improved),
        cummean(eta_hessian))
names<-c(rep("beta[0]",3*m),rep("beta[1]",3*m),rep("eta",3*m))
candidate<-c(rep("naive",m),rep("improved",m),rep("hessian",m))%>%rep(3)

df_plot<-tibble(pars,names,candidate,iteration=rep(1:m,9))%>%
  mutate(candidate=fct_relevel(candidate,"naive","improved","hessian"))

```

```{r echo=FALSE,message=FALSE}
ggplot(filter(df_plot,names == "beta[0]"),aes(x=iteration,y=pars))+
  geom_line()+
  facet_wrap(~candidate,labeller = label_parsed)+
  ggtitle(expression(beta[0]*"|"*bold(y))) +
  theme(plot.title = element_text(hjust = 0.5))+
  transition_reveal(along = iteration)
```

```{r echo=FALSE,message=FALSE}
ggplot(filter(df_plot,names == "beta[1]"),aes(x=iteration,y=pars))+
  geom_line()+
  facet_wrap(~candidate,labeller = label_parsed)+
  ggtitle(expression(beta[1]*"|"*bold(y))) +
  theme(plot.title = element_text(hjust = 0.5))+
  transition_reveal(along = iteration)
```

```{r echo=FALSE,message=FALSE}
ggplot(filter(df_plot,names == "eta"),aes(x=iteration,y=pars))+
  geom_line()+
  facet_wrap(~candidate,labeller = label_parsed)+
  ggtitle(expression(eta*"|"*bold(y))) +
  theme(plot.title = element_text(hjust = 0.5))+
  transition_reveal(along = iteration)
```


```{r echo=FALSE}
b0_hat_naive<-colMeans(beta_naive)[1]%>%round(3)
b1_hat_naive<-colMeans(beta_naive)[2]%>%round(3)
eta_hat_naive<-mean(eta_naive)%>%round(3)

b0_hat_improve<-colMeans(beta_improved)[1]%>%round(3)
b1_hat_improve<-colMeans(beta_improved)[2]%>%round(3)
eta_hat_improve<-mean(eta_improved)%>%round(3)

b0_hat_hess<-colMeans(beta_hessian)[1]%>%round(3)
b1_hat_hess<-colMeans(beta_hessian)[2]%>%round(3)
eta_hat_hess<-mean(eta_hessian)%>%round(3)
```

The MLE estimates are $\beta_0=$ `r ANS$par[1]%>%round(3)` $\beta_1=$ `r ANS$par[2]%>%round(3)` and $\eta=$ `r ANS$par[3]%>%round(3)`.  The posterior means for the various candidates are

:::{.table-narrow}
| candidate | $\beta_0$         | $\beta_1$         | $\eta$            |
------------|-------------------|-------------------|-------------------|
|naive      | `r b0_hat_naive`  |  `r b1_hat_naive` |`r eta_hat_naive`  |
|improved   | `r b0_hat_improve`| `r b1_hat_improve`|`r eta_hat_improve`|
|Hessian    | `r b0_hat_hess`   | `r b1_hat_hess`   |`r eta_hat_hess`   |

:::

The samples produced by these three candidates are all seem reasonable, but the cummulative means of $\beta$s produced using the naive candidate definitely do not look as good as the ones using the improved and the Hessian candidates.  The naive candidate seems to wander a great deal before finally settling down with values very different than those produced using the improved and Hessian candidates.  The improved and Hessian candidates produce sequences (or chains) that settle into a stationary distribution centered at what are reasonable values.  The $\eta$ estimates are likewise good for the improved and Hessian candidates, but while the samples from the naive candidate do appear to converge, it is to a pretty poor estimate.

From the looks of this it is a fair assessment to recommend discarding about the first $5000$

::::

### The Brooks-Gellman-Rubin Potential Scale Reduction Factor

:::{.sidenote}
**Example:**
Consider a sample
$\mathbf{y}=y_1,\ldots,y_n$ from $Y\sim Neg-Binom(\boldsymbol{\mu},\eta)$ where
$\log(\mu_i)=\beta_0+\beta_1x_i$ for some covariate $x_i$.

For $y=$ `r y` and $x=$ `r round(x,2)` fit the generlised linear model assuming the data are from a negative-binomial distribution using the priors
\begin{eqnarray}
\pi(\boldsymbol{\beta})&\propto& 1\\
\pi(\eta)&=&e^{-\eta}
\end{eqnarray}

Using the naive, improved, and Hessian candidates, draw $1000$ samples from three separate starting points from the posteriors for $\beta_0$ and $\beta_1$ and plot the cummulative means and compute the BGR statistic after $10,\!000$ samples.

:::

Graphically monitoring convergence has its drawbacks. First, graphical
methods rely on subjective interpretation of a visual representation of
the data. Second, it is conceivable that posterior distribution may have
more than one mode and the MCMC scheme may become trapped in a local
mode, then monitoring the cumulative mean may indicate false
convergence. Alternatively, it makes sense to monitor convergence from
multiple diverse initial conditions to ensure that the MCMC scheme is
robust and that evidence of convergence is representative of reality.

One such tool for monitoring the robustness as well as the convergence
of an MCMC scheme is the Brooks-Gelman-Rubin potential scale reduction
factor, which compares the within and between chain variances. For
over-dispersed initial conditions $\hat{R}$, the Brooks-Gelman-Rubin
potential scale reduction factor approaches unity from above, and is
implemented as follows:

:::{.boxed}
**The Brooks-Gelman-Rubin (BGR) potential scale reduction factor:**\

For $m$ independent chains from diverse starting points, compute the
variance estimate for each chain $j$:
$$
s^2_j=\frac{1}{n-1}\sum_{i=1}^n(\theta_{ij}-\bar{\theta}_j)^2
$$
and
let
$$
W=\frac{1}{m}\sum_{j=1}^ms^2_j
$$
be the average of the variance
across the chains and define the overall mean as
$$
\bar{\bar{\theta}}=\frac{1}{m}\sum_{j=1}^m\bar{\theta}_j
$$
and
$$
B=\frac{n}{m-1}\sum_{j=1}^m(\bar{\theta}_j-\bar{\bar{\theta}})^2
$$
and
$$
\widehat{\operatorname{Var}(\theta)}=\left(1-\frac{1}{n}\right)W+\frac{1}{n}B
$$
Then the B-G-R potential scale reduction factor is
$$
\hat{R}=\sqrt{\frac{\widehat{\operatorname{Var}(\theta)}}{W}}.
$$
Ploting the chains together is a useful tool for graphcially assessing whether or not the chains have converged, but calculating $\hat{R}$ is a bit more precise means of assessing convergence. 
Ideally, $\hat{R}$ should be close to $1$, and as a rule of thumb, a value of $\hat{R}<1.1$ is considered sufficiently close to $1$.  

:::

:::{.boxed}

###   Example {.tabset .tabset-pills}
Consider a sample
$\mathbf{y}=y_1,\ldots,y_n$ from $Y\sim Neg-Binom(\boldsymbol{\mu},\eta)$ where
$\log(\mu_i)=\beta_0+\beta_1x_i$ for some covariate $x_i$.

For $y=$ `r y` and $x=$ `r round(x,2)` fit the generlised linear model assuming the data are from a negative-binomial distribution using the priors
\begin{eqnarray}
\pi(\boldsymbol{\beta})&\propto& 1\\
\pi(\eta)&=&e^{-\eta}
\end{eqnarray}

Using the naive, improved, and Hessian candidates, draw $10,\!000$ samples from three separate starting points from the posteriors for $\beta_0$ and $\beta_1$ and plot the cummulative means and compute the BGR statistic after $10,\!000$ samples.

####    Solution

The previous examples show code for these three models, the difference here is that we are going to run each model three separate times from three difference initial values for the paramters.

####    Code

The easiest way to implement multiple chains is to write the Gibbs samplers as functions and then call each function three times.

```{r}

set.seed(14061972)

##  Define Function for log-likelihood, this will be used by all samplers

log_like<-function(BETA,ETA)
{
  mu<-X%*%BETA%>%exp()
  dnbinom(y,size = ETA,mu = mu, log = TRUE)%>%sum()
}

##  Define function for Gibbs sampler with the naive candidate

glm.naive<-function(x,y,n.samples,beta.init,eta.init)
{
  X<-cbind(1,x)
  m<-n.samples

  beta<-matrix(NA,ncol=2,nrow=m)
  eta<-numeric(length = m)

  beta[1,]<-beta.init
  eta[1]<-eta.init

  q_sd<-0.01
  beta_cov<-diag(q_sd,2)

  eta_var<-2

  for(i in 2:m)
  {
  ##  update betas

    bc<-rmvnorm(1,mean = beta[i-1,],sigma = beta_cov)%>%t()
    alpha<-(log_like(bc,eta[i-1])-log_like(beta[i-1,],eta[i-1]))%>%exp()
    u<-runif(1)
    if(alpha>u)
    {
      beta[i,]<-bc
    }
    else
    {
      beta[i,]<-beta[i-1,]
    }

  ##  update eta

    ac<-eta[i-1]^2/eta_var
    bc<-eta[i-1]/eta_var

    ec<-rgamma(1,ac,bc)

    a<-ec^2/eta_var
    b<-ec/eta_var

    alpha<-(log_like(beta[i,],ec)-log_like(beta[i,],eta[i-1])+dgamma(eta[i-1],a,b,log=TRUE)-dgamma(ec,ac,bc,log = TRUE)-ec+eta[i-1])%>%exp()

  u<-runif(1)
  if(alpha>u)
  {
    eta[i]<-ec
  }
  else
  {
    eta[i]<-eta[i-1]
  }

  }
  return(list(beta=beta,eta=eta))
}

##  Run for three chains
beta.init<-rbind(c(4.7,-1.3),c(0,1),c(0,0))
eta.init<-c(1,1,1)

naive.results<-list()

for(j in 1:3)
{
  set.seed(14061972)
  naive.results[[j]]<-glm.naive(x,y,10000,beta.init[j,],eta.init[j])
}

beta_0_naive<-c(naive.results[[1]]$beta[,1],naive.results[[2]]$beta[,1],naive.results[[3]]$beta[,1])

beta_1_naive<-c(naive.results[[1]]$beta[,2],naive.results[[2]]$beta[,2],naive.results[[3]]$beta[,2])

eta_naive<-c(naive.results[[1]]$eta,naive.results[[2]]$eta,naive.results[[3]]$eta)

chain<-rep(c(rep(1,10000),rep(2,10000),rep(3,10000)),3)%>%as.factor()

iteration<-rep(rep(1:10000,3),3)

names<-c(rep("beta[0]",3*10000),rep("beta[1]",3*10000),rep("eta",3*10000))

df_naive_bgr<-tibble(pars = c(beta_0_naive, beta1= beta_1_naive,eta_naive),chain,iteration,names)

```

```{r echo=FALSE}

set.seed(14061972)

glm.improved<-function(x,y,n.samples,beta.init,eta.init)
{
  X<-cbind(1,x)
  m<-n.samples

  beta<-matrix(NA,ncol=2,nrow=m)
  eta<-numeric(length = m)

  beta[1,]<-beta.init
  eta[1]<-eta.init

  q_sd<-2
beta_cov<-q_sd*solve(t(X)%*%X)

eta_var<-2

for(i in 2:m)
{
  ##  update betas
  bc<-rmvnorm(1,mean = beta[i-1,],sigma = beta_cov)%>%t()
  alpha<-(log_like(bc,eta[i-1])-log_like(beta[i-1,],eta[i-1]))%>%exp()
  u<-runif(1)
  if(alpha>u)
  {
    beta[i,]<-bc
  }
  else
  {
    beta[i,]<-beta[i-1,]
  }

  ##  update eta
  ac<-eta[i-1]^2/eta_var
  bc<-eta[i-1]/eta_var

  ec<-rgamma(1,ac,bc)

  a<-ec^2/eta_var
  b<-ec/eta_var

  alpha<-(log_like(beta[i,],ec)-log_like(beta[i,],eta[i-1])+dgamma(eta[i-1],a,b,log=TRUE)-dgamma(ec,ac,bc,log = TRUE)-ec+eta[i-1])%>%exp()

u<-runif(1)
if(alpha>u)
{
  eta[i]<-ec
}
else
{
  eta[i]<-eta[i-1]
}

}
  return(list(beta=beta,eta=eta))
}

improved.results<-list()

for(j in 1:3)
{
  set.seed(14061972)
  improved.results[[j]]<-glm.improved(x,y,10000,beta.init[j,],eta.init[j])
}

beta_0_improved<-c(improved.results[[1]]$beta[,1],improved.results[[2]]$beta[,1],improved.results[[3]]$beta[,1])

beta_1_improved<-c(improved.results[[1]]$beta[,2],improved.results[[2]]$beta[,2],improved.results[[3]]$beta[,2])

eta_improved<-c(improved.results[[1]]$eta,improved.results[[2]]$eta,improved.results[[3]]$eta)

chain<-rep(c(rep(1,10000),rep(2,10000),rep(3,10000)),3)%>%as.factor()

iteration<-rep(rep(1:10000,3),3)

names<-c(rep("beta[0]",3*10000),rep("beta[1]",3*10000),rep("eta",3*10000))

df_improved_bgr<-tibble(pars = c(beta_0_improved,beta_1_improved,eta_improved),chain,iteration,names)
```

```{r echo=FALSE}

set.seed(14061972)

glm.hessian<-function(x,y,n.samples,beta.init,eta.init)
{
  X<-cbind(1,x)
  m<-n.samples

  beta<-matrix(NA,ncol=2,nrow=m)
  eta<-numeric(length = m)

  beta[1,]<-beta.init
  eta[1]<-eta.init

q_sd<-3
beta_cov<-q_sd*H

eta_var<-2

for(i in 2:m)
{
  ##  update betas
  bc<-rmvnorm(1,mean = beta[i-1,],sigma = beta_cov)%>%t()
  alpha<-(log_like(bc,eta[i-1])-log_like(beta[i-1,],eta[i-1]))%>%exp()
  u<-runif(1)
  if(alpha>u)
  {
    beta[i,]<-bc
  }
  else
  {
    beta[i,]<-beta[i-1,]
  }

  ##  update eta
  ac<-eta[i-1]^2/eta_var
  bc<-eta[i-1]/eta_var

  ec<-rgamma(1,ac,bc)

  a<-ec^2/eta_var
  b<-ec/eta_var

  alpha<-(log_like(beta[i,],ec)-log_like(beta[i,],eta[i-1])+dgamma(eta[i-1],a,b,log=TRUE)-dgamma(ec,ac,bc,log = TRUE)-ec+eta[i-1])%>%exp()

u<-runif(1)
if(alpha>u)
{
  eta[i]<-ec
}
else
{
  eta[i]<-eta[i-1]
}

}
  return(list(beta=beta,eta=eta))
}

hessian.results<-list()

for(j in 1:3)
{
  set.seed(14061972)
  hessian.results[[j]]<-glm.hessian(x,y,10000,beta.init[j,],eta.init[j])
}

beta_0_hessian<-c(hessian.results[[1]]$beta[,1],hessian.results[[2]]$beta[,1],hessian.results[[3]]$beta[,1])

beta_1_hessian<-c(hessian.results[[1]]$beta[,2],hessian.results[[2]]$beta[,2],hessian.results[[3]]$beta[,2])

eta_hessian<-c(hessian.results[[1]]$eta,hessian.results[[2]]$eta,hessian.results[[3]]$eta)

chain<-rep(c(rep(1,10000),rep(2,10000),rep(3,10000)),3)%>%as.factor()

iteration<-rep(rep(1:10000,3),3)

names<-c(rep("beta[0]",3*10000),rep("beta[1]",3*10000),rep("eta",3*10000))

df_hessian_bgr<-tibble(pars = c(beta_0_hessian,beta_1_hessian,eta_hessian),chain,iteration, names)

```

####    Plot

The following GIFs show the animated paths of the traces for the three chains making up the BGR statistics 

```{r echo=FALSE,message=FALSE}
nb0<-filter(df_naive_bgr,names=="beta[0]")
ib0<-filter(df_improved_bgr,names=="beta[0]")
hb0<-filter(df_hessian_bgr,names=="beta[0]")

df_b0<-rbind(nb0,ib0,hb0)

nb1<-filter(df_naive_bgr,names=="beta[1]")
ib1<-filter(df_improved_bgr,names=="beta[1]")
hb1<-filter(df_hessian_bgr,names=="beta[1]")

df_b1<-rbind(nb1,ib1,hb1)

ne<-filter(df_naive_bgr,names=="eta")
ie<-filter(df_improved_bgr,names=="eta")
he<-filter(df_hessian_bgr,names=="eta")

df_e<-rbind(ne,ie,he)

candidate<-c(rep("naive",30000),rep("improved",30000),rep("hessian",30000))

df_b0$candidate<-candidate
df_b1$candidate<-candidate
df_e$candidate<-candidate

df_b0<-df_b0%>%
  mutate(candidate=fct_relevel(candidate,"naive","improved","hessian"))
df_b1<-df_b1%>%
  mutate(candidate=fct_relevel(candidate,"naive","improved","hessian"))
df_e<-df_e%>%
  mutate(candidate=fct_relevel(candidate,"naive","improved","hessian"))

```

See the animation for $\beta_0$

```{r echo=FALSE, message=FALSE}
ggplot(df_b0,aes(x=iteration,y=pars,group=chain,color=chain))+
  geom_line()+
  facet_wrap(~candidate,scales = "free")+
  ggtitle(expression(beta[0]*"|"*bold(y))) +
  theme(plot.title = element_text(hjust = 0.5))+
  transition_reveal(iteration)
```


```{r echo=FALSE, message=FALSE}
ggplot(df_b1,aes(x=iteration,y=pars,group=chain,color=chain))+
  geom_line()+
  facet_wrap(~candidate,scales = "free")+
  ggtitle(expression(beta[1]*"|"*bold(y))) +
  theme(plot.title = element_text(hjust = 0.5))+
  transition_reveal(iteration)

```


```{r echo=FALSE}

##  BGR Computations

### Naive beta_0

beta_0_1<-filter(df_naive_bgr,names=="beta[0]" & chain == 1)$pars
beta_0_2<-filter(df_naive_bgr,names=="beta[0]" & chain == 2)$pars
beta_0_3<-filter(df_naive_bgr,names=="beta[0]" & chain == 3)$pars

##  Chain means

mean_0_1<-mean(beta_0_1)
mean_0_2<-mean(beta_0_2)
mean_0_3<-mean(beta_0_3)

##  Within Chain Variance

var_0_1<-var(beta_0_1)
var_0_2<-var(beta_0_2)
var_0_3<-var(beta_0_3)

W<-mean(var_0_1,var_0_2,var_0_3)

GM<-mean(c(beta_0_1,beta_0_2,beta_0_3))

SIG<-(1-1/10000)*W+var(c(mean_0_1,mean_0_2,mean_0_3))

VHAT<-SIG+var(c(mean_0_1,mean_0_2,mean_0_3))/3

R_0_naive<-sqrt(VHAT/W)

### Naive beta_1

beta_0_1<-filter(df_naive_bgr,names=="beta[1]" & chain == 1)$pars
beta_0_2<-filter(df_naive_bgr,names=="beta[1]" & chain == 2)$pars
beta_0_3<-filter(df_naive_bgr,names=="beta[1]" & chain == 3)$pars

##  Chain means

mean_0_1<-mean(beta_0_1)
mean_0_2<-mean(beta_0_2)
mean_0_3<-mean(beta_0_3)

##  Within Chain Variance

var_0_1<-var(beta_0_1)
var_0_2<-var(beta_0_2)
var_0_3<-var(beta_0_3)

W<-mean(var_0_1,var_0_2,var_0_3)

GM<-mean(c(beta_0_1,beta_0_2,beta_0_3))

SIG<-(1-1/10000)*W+var(c(mean_0_1,mean_0_2,mean_0_3))

VHAT<-SIG+var(c(mean_0_1,mean_0_2,mean_0_3))/3

R_1_naive<-sqrt(VHAT/W)

##  BGR Computations

### improved beta_0

beta_0_1<-filter(df_improved_bgr,names=="beta[0]" & chain == 1)$pars
beta_0_2<-filter(df_improved_bgr,names=="beta[0]" & chain == 2)$pars
beta_0_3<-filter(df_improved_bgr,names=="beta[0]" & chain == 3)$pars

##  Chain means

mean_0_1<-mean(beta_0_1)
mean_0_2<-mean(beta_0_2)
mean_0_3<-mean(beta_0_3)

##  Within Chain Variance

var_0_1<-var(beta_0_1)
var_0_2<-var(beta_0_2)
var_0_3<-var(beta_0_3)

W<-mean(var_0_1,var_0_2,var_0_3)

GM<-mean(c(beta_0_1,beta_0_2,beta_0_3))

SIG<-(1-1/10000)*W+var(c(mean_0_1,mean_0_2,mean_0_3))

VHAT<-SIG+var(c(mean_0_1,mean_0_2,mean_0_3))/3

R_0_improved<-sqrt(VHAT/W)

### Naive beta_1

beta_0_1<-filter(df_improved_bgr,names=="beta[1]" & chain == 1)$pars
beta_0_2<-filter(df_improved_bgr,names=="beta[1]" & chain == 2)$pars
beta_0_3<-filter(df_improved_bgr,names=="beta[1]" & chain == 3)$pars

##  Chain means

mean_0_1<-mean(beta_0_1)
mean_0_2<-mean(beta_0_2)
mean_0_3<-mean(beta_0_3)

##  Within Chain Variance

var_0_1<-var(beta_0_1)
var_0_2<-var(beta_0_2)
var_0_3<-var(beta_0_3)

W<-mean(var_0_1,var_0_2,var_0_3)

GM<-mean(c(beta_0_1,beta_0_2,beta_0_3))

B<-(mean_0_1-GM)^2+(mean_0_2-GM)^2+(mean_0_3-GM)^2
B<-B*5000

VAR<-(1-1/10000)*W+B/10000

R_1_improved<-sqrt(VAR/W)



##  BGR Computations

### hessian beta_0

beta_0_1<-filter(df_hessian_bgr,names=="beta[0]" & chain == 1)$pars
beta_0_2<-filter(df_hessian_bgr,names=="beta[0]" & chain == 2)$pars
beta_0_3<-filter(df_hessian_bgr,names=="beta[0]" & chain == 3)$pars

##  Chain means

mean_0_1<-mean(beta_0_1)
mean_0_2<-mean(beta_0_2)
mean_0_3<-mean(beta_0_3)

##  Within Chain Variance

var_0_1<-var(beta_0_1)
var_0_2<-var(beta_0_2)
var_0_3<-var(beta_0_3)

W<-mean(var_0_1,var_0_2,var_0_3)

GM<-mean(c(beta_0_1,beta_0_2,beta_0_3))

B<-(mean_0_1-GM)^2+(mean_0_2-GM)^2+(mean_0_3-GM)^2
B<-B*5000

VAR<-(1-1/10000)*W+B/10000

R_0_hessian<-sqrt(VAR/W)

### Naive beta_1

beta_0_1<-filter(df_hessian_bgr,names=="beta[1]" & chain == 1)$pars
beta_0_2<-filter(df_hessian_bgr,names=="beta[1]" & chain == 2)$pars
beta_0_3<-filter(df_hessian_bgr,names=="beta[1]" & chain == 3)$pars

##  Chain means

mean_0_1<-mean(beta_0_1)
mean_0_2<-mean(beta_0_2)
mean_0_3<-mean(beta_0_3)

##  Within Chain Variance

var_0_1<-var(beta_0_1)
var_0_2<-var(beta_0_2)
var_0_3<-var(beta_0_3)

W<-mean(var_0_1,var_0_2,var_0_3)

GM<-mean(c(beta_0_1,beta_0_2,beta_0_3))

B<-(mean_0_1-GM)^2+(mean_0_2-GM)^2+(mean_0_3-GM)^2
B<-B*5000

VAR<-(1-1/10000)*W+B/10000

R_1_hessian<-sqrt(VAR/W)

R_0_naive%<>%round(3)
R_0_improved%<>%round(3)
R_0_hessian%<>%round(3)
R_1_naive%<>%round(3)
R_1_improved%<>%round(3)
R_1_hessian%<>%round(3)

```
The values for $\hat{R}$ are given in the table below

:::{.table-narrow}
| candidate | $\hat{R}(\beta_0)$| $\hat{R}(\beta_1)$|
------------|-------------------|-------------------|
|naive      | `r R_0_naive`     | `r R_1_naive`     |
|improved   | `r R_0_improved`  | `r R_1_improved`  |
|Hessian    | `r R_0_hessian`   | `r R_1_hessian`   |

:::

Reviewing the plots and the $\hat{R}$ values, it appears that there are some significant problesm with the naive candidate in terms of converging to the stationary posterior distribution. 

:::

##    Measuring Monte Carlo Error: Autocorrelation and Effective Sample Size

The Metropolis-Hastings algorithm only accepts proposal values with an
acceptance probability that is less than or equal to 1; resulting in an
overall acceptance rate of less than one, meaning that some sequential
values will be identical. In the case of the random walk version of the
Metropolis-Hastings algorithm, new candidates are conditioned on the
current value. So in both cases, sequential values exhibit some degree
of correlation or more precisely serial autocorrelation. As a result,
the MCMC samples are not perfectly independent, and inferential
assumptions of random sampling will underestimate the MCMC error. In
other words, the accuracy of estimates from the sample is reduced when
the samples are correlated. This reduction is important in MCMC
applications (as in any sampling problem), so it is encouraged to
measure and identify the effects of this correlation.

### Autocorrelation Function


The autocorrelation between two samples in an MCMC chain is the Pearson
correlation coefficient between the two samples. It is a function of the
lag or number of samples between them.
$$
\operatorname{Cor}\left(\theta^{(t)},\theta^{(t+k)}\right)=\rho^k.
$$
The
autocorrelation can be easily calculated and plotted to gain some
understanding of the degree of correlation present in the sequence.



###   Effective Sample Size

:::{.sidenote}
**Example:**
Consider a sample
$\mathbf{y}=y_1,\ldots,y_n$ from $Y\sim Neg-Binom(\boldsymbol{\mu},\eta)$ where
$\log(\mu_i)=\beta_0+\beta_1x_i$ for some covariate $x_i$.

For $y=$ `r y` and $x=$ `r round(x,2)` fit the generlised linear model assuming the data are from a negative-binomial distribution using the priors
\begin{eqnarray}
\pi(\boldsymbol{\beta})&\propto& 1\\
\pi(\eta)&=&e^{-\eta}
\end{eqnarray}

Using the naive, improved, and Hessian candidates, draw $10,\!000$ samples from the posteriors for $\beta_0$ and $\beta_1$ and plot autocorrelation functions and compute the effective sample sizes.

:::

More
accurately, given the autocorrelation function $\rho^k$ can be used to
compute the *effective* sample size that is less than the actual sample
size $n$.
$$
ESS=\frac{n}{1+2\sum_{k=1}^\infty\rho_k}.
$$
The effective
sample size is a more quantitative measure of the efficiency of MCMC
schemes and can be a useful measure of the reliability of estimates
based on the samples.

:::{.boxed}
###   Example {.tabset .tabset-pills}

Consider a sample
$\mathbf{y}=y_1,\ldots,y_n$ from $Y\sim Neg-Binom(\boldsymbol{\mu},\eta)$ where
$\log(\mu_i)=\beta_0+\beta_1x_i$ for some covariate $x_i$.

For $y=$ `r y` and $x=$ `r round(x,2)` fit the generlised linear model assuming the data are from a negative-binomial distribution using the priors
\begin{eqnarray}
\pi(\boldsymbol{\beta})&\propto& 1\\
\pi(\eta)&=&e^{-\eta}
\end{eqnarray}

Using the naive, improved, and Hessian candidates, draw $10,\!000$ samples from the posteriors for $\beta_0$ and $\beta_1$ and plot autocorrelation functions and compute the effective sample sizes.

####    Solution

In practice there are several function in `R` that automatically compute the ACF, we will use `acf()` and `ggplot` to create our ACF plots. 

####    Code

```{r}
library(mcmcse)

ess_naive_beta_0<-filter(df_naive_bgr,names=="beta[0]" & chain == 2)$pars%>%ess()%>%round(2)
ess_improved_beta_0<-filter(df_improved_bgr,names=="beta[0]" & chain == 2)$pars%>%ess()%>%round(2)
ess_hessian_beta_0<-filter(df_hessian_bgr,names=="beta[0]" & chain == 2)$pars%>%ess()%>%round(2)

ess_naive_beta_1<-filter(df_naive_bgr,names=="beta[1]" & chain == 2)$pars%>%ess()%>%round(2)
ess_improved_beta_1<-filter(df_improved_bgr,names=="beta[1]" & chain == 2)$pars%>%ess()%>%round(2)
ess_hessian_beta_1<-filter(df_hessian_bgr,names=="beta[1]" & chain == 2)$pars%>%ess()%>%round(2)

acf_naive_beta_0<-filter(df_naive_bgr,names=="beta[0]" & chain == 2)$pars%>%acf(plot=FALSE)
acf_improved_beta_0<-filter(df_improved_bgr,names=="beta[0]" & chain == 2)$pars%>%acf(plot=FALSE)
acf_hessian_beta_0<-filter(df_hessian_bgr,names=="beta[0]" & chain == 2)$pars%>%acf(plot=FALSE)

acf_naive_beta_1<-filter(df_naive_bgr,names=="beta[1]" & chain == 2)$pars%>%acf(plot=FALSE)
acf_improved_beta_1<-filter(df_improved_bgr,names=="beta[1]" & chain == 2)$pars%>%acf(plot=FALSE)
acf_hessian_beta_1<-filter(df_hessian_bgr,names=="beta[1]" & chain == 2)$pars%>%acf(plot=FALSE)

```

####    Plot

```{r echo=FALSE}

df_acf_naive<-tibble(acf=c(acf_naive_beta_0$acf,acf_naive_beta_1$acf),lag=rep(acf_naive_beta_0$lag,2),names=c(rep("beta[0]",41),rep("beta[1]",41)))

ggplot(df_acf_naive,aes(x=lag,y=acf))+
  geom_bar(stat = "identity")+
  facet_wrap(~names,labeller = label_parsed)+
  ylab("autocorrelation factor")+
  xlab("lag")
```

```{r echo=FALSE}

##  Improved

df_acf_improved<-tibble(acf=c(acf_improved_beta_0$acf,acf_improved_beta_1$acf),lag=rep(acf_improved_beta_0$lag,2),names=c(rep("beta[0]",41),rep("beta[1]",41)))

ggplot(df_acf_improved,aes(x=lag,y=acf))+
  geom_bar(stat = "identity")+
  facet_wrap(~names,labeller = label_parsed)+
  ylab("autocorrelation factor")+
  xlab("lag")

```

```{r echo=FALSE}

##  Hessian

df_acf_hessian<-tibble(acf=c(acf_hessian_beta_0$acf,acf_hessian_beta_1$acf),lag=rep(acf_hessian_beta_0$lag,2),names=c(rep("beta[0]",41),rep("beta[1]",41)))

ggplot(df_acf_improved,aes(x=lag,y=acf))+
  geom_bar(stat = "identity")+
  facet_wrap(~names,labeller = label_parsed)+
  ylab("autocorrelation factor")+
  xlab("lag")

```

The resulting effective sample sizes 

:::{table-narrow}
| candidate | $ESS(\beta_0)$| $ESS(\beta_1)$|
------------|------------------------|------------------------|
|naive      | `r ess_naive_beta_0`   |`r ess_naive_beta_1`    |
|improved   | `r ess_improved_beta_0`| `r ess_improved_beta_1`|
|Hessian    | `r ess_hessian_beta_0` | `r ess_hessian_beta_1` |

:::
 and ACF plots reinforce the conclusion that the naive candidate does not produce good samples. 

:::

##    Deviance Information Criteria {#deviance-information-criteria .unnumbered}

:::{.sidenote}
**Example:**
Consider a sample
$\mathbf{y}=y_1,\ldots,y_n$ from $Y\sim Neg-Binom(\boldsymbol{\mu},\eta)$ where
$\log(\mu_i)=\beta_0+\beta_1x_i$ for some covariate $x_i$.

For $y=$ `r y` and $x=$ `r round(x,2)` fit the generlised linear model assuming the data are from a negative-binomial distribution using the priors
\begin{eqnarray}
\pi(\boldsymbol{\beta})&\propto& 1\\
\pi(\eta)&=&e^{-\eta}
\end{eqnarray}

Using the naive, improved, and Hessian candidates, draw $10,\!000$ samples from the posteriors for $\beta_0$ and $\beta_1$ and compute the DIC and p$_d$ for each.
:::

The deviance information criterion (DIC) is an information criterion
much like the AIC or BIC. It is a measure that takes into account both
the model complexity and goodness of fit in order for use as a model
selection criteria. Defining the deviance as
$$
D(\theta) = -2\log(f(\mathbf{y}|\theta))
$$
or $-2$ times the
log-likelihood, it is a measure of goodness of fit. Model complexity can
also be measured in terms of deviance by one of two measures:
$$
p_D= \overline{D(\theta)}-D(\bar{\theta})
$$
where
$\overline{D(\theta)}$ is the expected value of the deviance, and
$D(\bar{\theta})$ is the deviance evaluated at the expected value of
$\theta$. Alternatively, the measure
$$
p_V = \frac{1}{2}\operatorname{Var}(D(\theta))
$$
can be used a a complexity measure, and the resulting DIC is
$$
DIC = p_D+\overline{D(\theta)},\mbox{ or }D(\bar{\theta})+2p_D
$$
with the second form highlighting the relationship with AIC. Note that $p_V$
can be substituted for $p_D$ if desired, but this is not standard in
most implementations.

One can implement the DIC in any Monte Carlo scheme desired,
$\overline{D(\theta)}$ and $p_V$ can be easily calculated by computing
the log-likelihood at each iteration and computing the mean and
variance. Calculating the $p_D$ consists of calculating the
log-likelihood using the posterior mean of $\theta$.

:::{.boxed}
###   Example {.tabset .tabset-pills}
Consider a sample
$\mathbf{y}=y_1,\ldots,y_n$ from $Y\sim Neg-Binom(\boldsymbol{\mu},\eta)$ where
$\log(\mu_i)=\beta_0+\beta_1x_i$ for some covariate $x_i$.

For $y=$ `r y` and $x=$ `r round(x,2)` fit the generlised linear model assuming the data are from a negative-binomial distribution using the priors
\begin{eqnarray}
\pi(\boldsymbol{\beta})&\propto& 1\\
\pi(\eta)&=&e^{-\eta}
\end{eqnarray}

Using the naive, improved, and Hessian candidates, draw $10,\!000$ samples from the posteriors for $\beta_0$ and $\beta_1$ and compute the DIC and p$_d$ for each.

####    Solution

Given the MCMC sampling scheme, the DIC and $p_D$ can be computed by calculating and recording the deviance at each iteration $t$
$$
D(\boldsymbol{\beta}_t,\eta_t)=-2\log\left\{\prod_{i=1}^n\frac{\Gamma(y_i+\eta_t)}{\Gamma(\eta_t)y_i!}\left(\frac{\eta_t}{\eta_t+\exp(\beta_{0,t}+\beta_{1,t}x_i)}\right)^{\eta_t}\times\\
\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\left(\frac{\exp(\beta_{0,t}+\beta_{1,t}x_i)}{\eta_t+\exp(\beta_{0,t}+\beta_{1,t}x_i)}\right)^{y_i}
\right\}
$$
to find 
$$
\overline{D(\boldsymbol{\beta},\eta)}=\frac{1}{T-t_0+1}\sum_{t=t_0}^T-2\log\left\{\prod_{i=1}^n\frac{\Gamma(y_i+\eta_t)}{\Gamma(\eta_t)y_i!}\left(\frac{\eta_t}{\eta_t+\exp(\beta_{0,t}+\beta_{1,t}x_i)}\right)^{\eta_t}\times\\
\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\left(\frac{\exp(\beta_{0,t}+\beta_{1,t}x_i)}{\eta_t+\exp(\beta_{0,t}+\beta_{1,t}x_i)}\right)^{y_i}
\right\}
$$
where $t_0$ is the initial value of $t$ after any samples are discarded for burin-in.

We can then find $D(\bar{\boldsymbol{\beta}},\bar{\eta})$ where
$$
\begin{align}
\bar{\beta}_0&=\frac{1}{t_0+1}\sum_{t=t_0}^T\beta_{0,t}\\
\bar{\beta}_1&=\frac{1}{t_0+1}\sum_{t=t_0}^T\beta_{1,t}\\
\bar{\eta}&=\frac{1}{t_0+1}\sum_{t=t_0}^T\eta_t
\end{align}
$$
then
$$
D(\bar{\boldsymbol{\beta}},\bar{\eta})=-2\log\left\{\prod_{i=1}^n\frac{\Gamma(y_i+\bar{\eta})}{\Gamma(\bar{\eta})y_i!}\left(\frac{\bar{\eta}} {\bar{\eta}+\exp(\bar{\beta_{0}}+\bar{\beta_{1}}x_i)}\right)^{\bar{\eta}}\times\\
\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\left(\frac{\exp(\bar{\beta_0}+\bar{\beta_1}x_i)}{\bar{\eta}+\exp(\bar{\beta_0}+\bar{\beta_1}x_i)}\right)^{y_i}
\right\}
$$
and 
$$
p_D=\overline{D(\boldsymbol{\beta},\eta)}-D(\bar{\boldsymbol{\beta}},\bar{\eta})
$$
and 
$$
DIC = p_D+\overline{D(\boldsymbol{\beta},\eta)}
$$

####    Code

We will look at three sets of code and the results to see the effects of the various candidate distributions on the $DIC$ and $p_D$ and $p_V$.

Example 1: Using the naive candidate

```{r}

library(mvtnorm)

set.seed(14061972)
x<-rnorm(15,10,1)
mu<-4.7*x-1.3
y<-rnbinom(15,mu=mu,size=1.6)

X<-cbind(1,x)

log_like<-function(BETA,ETA)
{
  mu<-X%*%BETA%>%exp()
  dnbinom(y,size = ETA,mu = mu, log = TRUE)%>%sum()
}

m<-10000

beta<-matrix(NA,ncol=2,nrow=m)
eta<-numeric(length = m)
dev<-numeric(length = m)

beta[1,]<-0
eta[1]<-1
dev[1]<--2*log_like(beta[1,],eta[1])

q_sd<-0.01
beta_cov<-diag(q_sd,2)

eta_var<-2

for(i in 2:m)
{
  ##  update betas
  bc<-rmvnorm(1,mean = beta[i-1,],sigma = beta_cov)%>%t()
  alpha<-(log_like(bc,eta[i-1])-log_like(beta[i-1,],eta[i-1]))%>%exp()
  u<-runif(1)
  if(alpha>u)
  {
    beta[i,]<-bc
  }
  else
  {
    beta[i,]<-beta[i-1,]
  }
  
  ##  update eta
  ac<-eta[i-1]^2/eta_var
  bc<-eta[i-1]/eta_var
  
  ec<-rgamma(1,ac,bc)
  
  a<-ec^2/eta_var
  b<-ec/eta_var
  
  alpha<-(log_like(beta[i,],ec)-log_like(beta[i,],eta[i-1])+dgamma(eta[i-1],a,b,log=TRUE)-dgamma(ec,ac,bc,log = TRUE)-ec+eta[i-1])%>%exp()

u<-runif(1)
if(alpha>u)
{
  eta[i]<-ec
}
else
{
  eta[i]<-eta[i-1]
}
dev[i]<--2*log_like(beta[i,],eta[i])
}

Dbar_naive<-mean(dev)
pD_naive<-Dbar_naive+2*log_like(colMeans(beta),mean(eta))
pV_naive<-var(dev)/2

```


$$
\phantom{linebreak}
$$
Example 2: Using the improved candidate

```{r}

library(mvtnorm)

set.seed(14061972)
x<-rnorm(15,10,1)
mu<-4.7*x-1.3
y<-rnbinom(15,mu=mu,size=1.6)

X<-cbind(1,x)

log_like<-function(BETA,ETA)
{
  mu<-X%*%BETA%>%exp()
  dnbinom(y,size = ETA,mu = mu, log = TRUE)%>%sum()
}

m<-10000

beta<-matrix(NA,ncol=2,nrow=m)
eta<-numeric(length = m)
dev<-numeric(length = m)

beta[1,]<-0
eta[1]<-1
dev[1]<--2*log_like(beta[1,],eta[1])

q_sd<-2
beta_cov<-q_sd*solve(t(X)%*%X)

eta_var<-2

for(i in 2:m)
{
  ##  update betas
  bc<-rmvnorm(1,mean = beta[i-1,],sigma = beta_cov)%>%t()
  alpha<-(log_like(bc,eta[i-1])-log_like(beta[i-1,],eta[i-1]))%>%exp()
  u<-runif(1)
  if(alpha>u)
  {
    beta[i,]<-bc
  }
  else
  {
    beta[i,]<-beta[i-1,]
  }

  ##  update eta
  ac<-eta[i-1]^2/eta_var
  bc<-eta[i-1]/eta_var

  ec<-rgamma(1,ac,bc)

  a<-ec^2/eta_var
  b<-ec/eta_var

  alpha<-(log_like(beta[i,],ec)-log_like(beta[i,],eta[i-1])+dgamma(eta[i-1],a,b,log=TRUE)-dgamma(ec,ac,bc,log = TRUE)-ec+eta[i-1])%>%exp()

u<-runif(1)
if(alpha>u)
{
  eta[i]<-ec
}
else
{
  eta[i]<-eta[i-1]
}
dev[i]<--2*log_like(beta[i,],eta[i])
}

Dbar_impro<-mean(dev)
pD_impro<-Dbar_impro+2*log_like(colMeans(beta),mean(eta))
pV_impro<-var(dev)/2

```
$$
\phantom{linebreak}
$$

Example 3: Using the Hessian candidate

```{r}

library(mvtnorm)

set.seed(14061972)
x<-rnorm(15,10,1)
mu<-4.7*x-1.3
y<-rnbinom(15,mu=mu,size=1.6)

X<-cbind(1,x)

log_like<-function(BETA,ETA)
{
  mu<-X%*%BETA%>%exp()
  dnbinom(y,size = ETA,mu = mu, log = TRUE)%>%sum()
}

ANS<-optim(c(0,0,1),function(x) -log_like(x[1:2],x[3]), lower = c(-100,-100,0), method = "L-BFGS-B", hessian = TRUE)

H<-solve(ANS$hessian[1:2,1:2])

m<-10000

beta<-matrix(NA,ncol=2,nrow=m)
eta<-numeric(length = m)
dev<-numeric(length = m)

beta[1,]<-0
eta[1]<-1
dev[1]<--2*log_like(beta[1,],eta[1])

q_sd<-3
beta_cov<-q_sd*H

eta_var<-2

for(i in 2:m)
{
  ##  update betas
  bc<-rmvnorm(1,mean = beta[i-1,],sigma = beta_cov)%>%t()
  alpha<-(log_like(bc,eta[i-1])-log_like(beta[i-1,],eta[i-1]))%>%exp()
  u<-runif(1)
  if(alpha>u)
  {
    beta[i,]<-bc
  }
  else
  {
    beta[i,]<-beta[i-1,]
  }

  ##  update eta
  ac<-eta[i-1]^2/eta_var
  bc<-eta[i-1]/eta_var

  ec<-rgamma(1,ac,bc)

  a<-ec^2/eta_var
  b<-ec/eta_var

  alpha<-(log_like(beta[i,],ec)-log_like(beta[i,],eta[i-1])+dgamma(eta[i-1],a,b,log=TRUE)-dgamma(ec,ac,bc,log = TRUE)-ec+eta[i-1])%>%exp()

u<-runif(1)
if(alpha>u)
{
  eta[i]<-ec
}
else
{
  eta[i]<-eta[i-1]
}
dev[i]<--2*log_like(beta[i,],eta[i])
}

Dbar_hessi<-mean(dev)
pD_hessi<-Dbar_hessi+2*log_like(colMeans(beta),mean(eta))
pV_hessi<-var(dev)/2

```

```{r echo=FALSE}
DICpD_naive<-(Dbar_naive+pD_naive)%>%round(2)
DICpV_naive<-(Dbar_naive+pV_naive)%>%round(2)
DICpD_impro<-(Dbar_impro+pD_impro)%>%round(2)
DICpV_impro<-(Dbar_impro+pV_impro)%>%round(2)
DICpD_hessi<-(Dbar_hessi+pD_hessi)%>%round(2)
DICpV_hessi<-(Dbar_hessi+pV_hessi)%>%round(2)
```

:::{table-narrow}

| candidate | $\bar{D}$      |$p_D$       |$p_v$       |$DIC_{p_D}$    |$DIC_{p_V}$    |
------------|----------------|------------|------------|---------------|---------------|
|naive      | `r Dbar_naive` |`r pD_naive`|`r pV_naive`|`r DICpD_naive`|`r DICpV_naive`|
|improved   | `r Dbar_impro` |`r pD_impro`|`r pV_impro`|`r DICpD_impro`|`r DICpV_impro`|
|Hessian    | `r Dbar_hessi` |`r pD_hessi`|`r pV_hessi`|`r DICpD_hessi`|`r DICpV_hessi`|

:::
The results for the DIC tell a mixed stroy, and have to be considered in light of all the other diagnostics.  While the overall fit for the naive candidate is worse than for either the improved or the Hessian candidates, the overall $DIC$ values are pretty similar, with naive candidate being the "preferred" model. Despite this we know that the naive candidate is not good, there is little evidence of convergence, and the effective sample sizes are very small. Despite the $DIC$ results we would _not_ choose the naive candidate results.   

The results for the improved and Hessian candidates are in agreement, as in the previous cases. While there is no formal statistical test for differences between $DIC$ values, it is fair to say that there is not any significant difference in the results for the improved and Hessian candidates; both appear to accurately fit the model and return good results in terms of DIC. 


:::

