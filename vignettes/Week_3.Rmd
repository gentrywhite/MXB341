---
title: "Week 3"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Week 3}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, echo=FALSE}

htmltools::includeHTML("header.html")
htmltools::includeCSS("QUTReadings.css")

```


# Asymptotic Properties of the Maximum Likelihood Estimator

Previously, we have looked at methods for estimating parameter, properties of estimators, identifying good estimators and comparing estimators.  Now we will look at the asymptotic properties of estimators as random variables, and the concepts of exponential families and deviance to identify and evaluate estimators.  

Estimators are functions of data, which are random variables, implying that estimators are also random variables. We can use this fact to make inference or probabilistic statements about estimators and their values, the strength of our belief in those values, and ultimately, perform statistical tests based on these estimators and explore decision making under uncertainty.  Often times the exact probability distribution of an estimator is unknown or intractable, in these cases, if we are using maximum likelihood estimators we can rely on their asymptotic properties to make inference. 

##    Convergence of Estimators

:::{.sidenote}
**Stochastic Convergence**\
Stochastic convergence describes the asymptotic behaviour of sequences of numbers generated by random variables.  Stochastic convergence differs from point-wise convergence that occurs in real analysis and falls into one of three categories:

**Convergence in Distribution:**\
A sequence of random variates $\mathbf{y}_n=y_1,y_2,\ldots,y_n$ is said to converge in distribution (or converge weakly or converge in law) if 
$$
\lim_{n\rightarrow\infty}F_n(y)=F(y),\,\forall y\in\mathbb{R}:F(y) \mbox{ is continuous}.
$$
where $F_n$ and $F$ are respectively the cumulative distribution functions for the sequence $\mathbf{y}_n$ and the random variable $Y$. An alternative notation is 
$$
\mathbf{y}_n\stackrel{d}{\rightarrow}Y.
$$

**Convergence in Probability:**\
A sequence $\mathbf{y}_n=y_1,y_2,\ldots,y_n$ is said to converge in probability to the random variable $Y$ if
$$
\lim_{n\rightarrow\infty}Pr(|\mathbf{y}_n-Y|>\epsilon)=0,\,\forall \epsilon>0.
$$
Convergence in probability implies convergence in distribution, but convergence in distribution only implies convergence in probability if $Y$ is a constant.  Convergence in probability has the alternative notation
$$
\mathbf{y}_n\stackrel{p}{\rightarrow}Y.
$$

**Almost Sure Convergence:**\
A sequence $\mathbf{y}_n=y_1,y_2,\ldots,y_n$ converges almost surely (or strongly) to the random variable $Y$ if
$$
Pr\left(\lim_{n\rightarrow\infty}\mathbf{y}_n=Y\right)=1.
$$
Almost sure convergence implies convergence in probability and distribution and has the alternative notation 
$$
\mathbf{y}_n\stackrel{a.s.}{\rightarrow}Y.
$$


:::

The first step to understanding the estimators' asymptotic behaviour is to look at some of the theorems describing the convergence behaviours of estimators to understand the formal approaches underpinning their behaviours.  

### The Central Limit Theorem

The Central Limit Theorem is shown in elementary statistics units to justify the use of $Z$ or $t$-tests for sample means, and as stated, is an example of *convergence in distribution*
$$
\sqrt{n}\left(\bar{y}-\operatorname{E}(Y)\right)\stackrel{d}{\rightarrow}N(0,\sigma^2)
$$
i.e.\ sufficiently large samples, we can assume that the sample mean follows a Gaussian distribution with the population mean and variance. This form of convergence is weak but is useful in establishing a basis for inference on the sample mean.  

### The Law of Large Numbers
The Law of Large Numbers for the sample mean is a stronger statement of convergence, as an example of *convergence in probability* 
$$
Pr(|\bar{y}-\operatorname{E}(Y)|<\epsilon)\rightarrow 0, n\rightarrow\infty, \forall \epsilon.
$$
In other words, if $Y$ is a random variable from a probability distribution with $\operatorname{E}(Y)$ then for a random sample $\mathbf{y}=y_1,y_2,\ldots,y_n$ from the probability distribution of $Y$, the sample mean $\bar{y}$ approaches $\operatorname{E}(Y)$ as $n$ approaches infinity.  

The Law of Large Numbers describes the behaviour of the sample mean of samples from a random variable with respect to the expected value of that random variable.  But we will shortly see that we can use the Law of Large Numbers to define consistency for any estimator, not just the sample mean.  

###    Consistency

[Consistency](./Properties_of_Estimators.html#consistency) has been defined previously, but here we offer a more rigorous definition and justification via the Law of Large Numbers.  An estimator $\hat{\theta}$ is said to be consistent if
$$
\hat{\theta}\stackrel{p}{\rightarrow}\theta_0
$$
for some true value $\theta_0$ of the parameter $\theta$.  

:::{.boxed}
**Theorem:**\
Given a sequence $\mathbf{y}_n=y_1,y_2,\ldots,y_n$ such that 
$$
\mathbf{y}_n\stackrel{p}{\rightarrow}Y
$$
then for any continuous function $h(\cdot)$ the sequence $h(y_1),h(y_2)\ldots,h(y_n)$ converges to $h(Y)$. 
:::

 Plainly stated, if a sequence of observations converges in probability to a random variable $Y$, then any function of those observations creates a new sequence that converges in probability to that same function of the random variable. We will see this used shortly to justify the consistency of the maximum likelihood estimator. 


##   Sampling Distribution of the MLE

To make inference about estimators, we need to know the probability distribution or sampling distribution of the estimator. Because estimators are functions of random variables, or more accurately, functions of data generated by random variables, estimators will have probability distributions.  Often, computing these probability distributions is an intractable task, and instead, we rely on these estimators' asymptotic distributions.  We can show specifically that the maximum likelihood estimator's asymptotic distribution has several nice properties that facilitate inference.

###    Consistency of the MLE

The formal definition of consistency is useful specifically as it provides a means of formal proof of the maximum likelihood estimator's property of consistency.  Building on the Theorem above, stating (subject to some conditions) that if a sequence of values  converges in probability to a given random variability, then any sequence of functions of that sequence will converge to the function of the random variable, we can prove that the maximum likelihood estimator is consistent as follows:

:::{.boxed}
**Proof:**\
Given the probability distribution $f(y;\theta)$ with some true but unknown value $\theta=\theta_0$ let
$$
\begin{align}
L_n(\theta)&=\frac{1}{n}\sum_{i=1}^n\log\left(f(y_i;\theta\right)\\
l(y|\theta)&=\log\left(f(y;\theta)\right)\\
L(\theta)&=\operatorname{E}_{\theta_0}\left(l(y|\theta)\right)\\
&=\int_Y\log(f(y;\theta))f(y;\theta_0)dx
\end{align}
$$
Note that $\hat{\theta}$ is the maximum likelihood estimator and maximises $L_n(\theta)$.

By the Law of Large Numbers
$$
L_n(\theta)\rightarrow\operatorname{E}_{\theta_0}\left(l(\theta|y)\right)
$$
**Lemma:**\
$$
L(\theta)\leq L(\theta_0)
$$
where the inequality is strictly less than unless
$$
Pr(f(y;\theta)=f(y;\theta_0))=1
$$
**Proof:**\
$$
\begin{align}
L(\theta)-L(\theta_0)&\leq 0\\
&=\operatorname{E}_{\theta_0}\left\{\log(f(y;\theta))-\log(f(y;\theta_0)\right\}\\
&=\operatorname{E}_{\theta_0}\left\{\log\left[\frac{f(y;\theta)}{f(y;\theta_0)}\right]\right\}
\end{align}
$$
Note that $\log(y)\leq y-1$
$$
\begin{align}
\operatorname{E}_{\theta_0}\left\{\log\left[\frac{f(y;\theta)}{f(y;\theta_0)}\right]\right\}&\leq
\operatorname{E}_{\theta_0}\left\{\frac{f(y;\theta)}{f(y;\theta_0)}-1\right\}\\
\operatorname{E}_{\theta_0}\left\{\frac{f(y;\theta)}{f(y;\theta_0)}-1\right\}&=\int_Y\left\{
\frac{f(y;\theta)}{f(y;\theta_0)}-1\right\}f(y;\theta_0)dx\\
&=\int_Yf(y;\theta)dx-\int_Yf(y;\theta_0)dx\\
&=1-1\\
&=0
\end{align}
$$
Note that both integrals evaluate to $1$ because they are probability density functions. This Lemma proves that $L(\theta)-L(\theta_0)\leq 0$ which leads to:

1. $\hat{\theta}$ is the maximiser of $L_n(\theta)$.
2. $\theta_0$ is the maximiser of $L(\theta$.
3. By the Law of Large Numbers $L_n(\theta)\rightarrow L(\theta)$

Since the two functions converge, so do their maximisers; hence the maximum likelihood estimator $\hat{\theta}$ converges to $\theta_0$ and is thus consistent. 

:::

This final "proof" of the maximum likelihood estimator's consistency is not rigorous but provides an intuitive argument for illustrative purposes.  

###  Asymptotic Normality of the MLE

We can state the proposition that the maximum likelihood estimator asymptotically follows a Gaussian (Normal) probability density function as
$$
\sqrt{n}(\hat{\theta}-\theta_0)\rightarrow N\left(0,\frac{1}{\mathcal{I}(\theta_0)}\right)
$$
where $\hat{\theta}$ is the maximum likelihood estimator of the unknown parameter $\theta_0$, and $\mathcal{I}(\theta_0)$ is the Fisher Information for $\theta$ evaluated at $\theta_0$. 

:::{.boxed}
**Proof:**\
Noting that $\hat{\theta}$ is the maximiser of $L_n(\theta)$ then $L'(\hat{\theta})=0$, or more explicitly
$$
\left.\frac{dL_n(\theta)}{d\theta}\right\vert_{\theta=\theta_0}=0.
$$
The Mean Value Theorem is
$$
\frac{f(a)-f(b)}{a-b}=f'(c)
$$
or
$$
f(a)=f(b)+f'(c)(a-b),\,\forall c\in[a,b].
$$
If we define $f(\theta)=L_n'(\theta)$, $a=\hat{\theta}$ and $b=\theta_0$. Then
$$
\begin{align}
L_n'(\hat{\theta})&=0\\
&=L_n(\theta_0)+L''_n(\hat{\theta}_1)(\hat{\theta}-\theta_0),\,\hat{\theta}_1\in[\hat{\theta},\theta_0]
\end{align}
$$
which can be re-written as
$$
\hat{\theta}-\theta_0=-\frac{L_n'(\theta_0)}{L_n''(\hat{\theta}_1)}
$$
or
$$
\sqrt{n}(\hat{\theta}-\theta_0)=\frac{\sqrt{n}L_n'(\theta_0)}{L_n''(\hat{\theta}_1)}.
$$
Previously, we have seen that $\hat{\theta}$ is the maximiser of $L(\theta)$, hence the numerator 
$$
\begin{align}
\sqrt{n}L_n'(\theta_0)&=\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^nl(y_i|\theta_0)-0\right)\\
&=\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^nl(y_i|\theta_0)-\operatorname{E_{\theta_0}(l'(y_1|\theta_0)}\right)\\
&\stackrel{d}{\rightarrow} N\left(0,\operatorname{Var}_{\theta_0}\left(l'(y_1|\theta_0)\right)\right)
\end{align}
$$
converges in distribution by the Central Limit Theorem. 

Now, the denominator 
$$
\begin{align}
L_n''(\theta)&=\frac{1}{n}\sum_{i=1}^nl''(y_i|\theta)\\
&\rightarrow\operatorname{E}_{\theta_0}\left(l''(y_1|\theta)\right)
\end{align}
$$
by the Law of Large Numbers.  Because $\hat{\theta}_1\in[\hat{\theta},\theta_0]$ and by the previous consistency result $\hat{\theta}\rightarrow\theta_0$ and $\hat{\theta}_1\rightarrow\theta_0$. Thus
$$
L_n''(\hat{\theta}_1)\rightarrow\operatorname{E}_{\theta_0}\left(l''(y_1|\theta_0)\right)=-\mathcal{I}(\theta_0).
$$
Combining the results for the numerator and the denominator
$$
\frac{\sqrt{n}L_n'(\theta_0)}{L_n''(\hat{\theta}_1)}\stackrel{d}{\rightarrow}N\left(0,\frac{\operatorname{Var_{\theta_0}(l'(y_1|\theta_0))}}{\left(\mathcal{I}(\theta_0)\right)^2}\right)
$$
and finally
$$
\begin{align}
\operatorname{Var}_{\theta_0}\left(l'(y|\theta_0)\right)&=\operatorname{E}_{\theta_0}\left\{\left(l;(y|\theta)\right)^2\right\}-\left\{\operatorname{E}_{\theta_0}(l'(y|\theta))\right\}^2\\
&=\mathcal{I}(\theta)-0
\end{align}
$$
hence 
$$
\frac{\sqrt{n}L_n'(\theta_0)}{L_n''(\hat{\theta}_1)}\stackrel{d}{\rightarrow}N\left(0,\frac{1}{\mathcal{I}(\theta_0)}\right).
$$

:::

So, by the two previous proofs, we can see that the maximum likelihood estimator $\hat{\theta}$ is both consistent and 
$$
\sqrt{n}(\hat{\theta}-\theta_0)\stackrel{d}{\rightarrow}N\left(0,\frac{1}{\mathcal{I}(\theta_0)}\right)
$$
converges in distribution to a Gaussian distribution with a mean of $\theta_0$ and variance of $\mathcal{I}(\theta_0)^{-1}$.  These results allow us to make probabilistic statements about the maximum likelihood estimator and formal statistical inference. 


# Exponential Families

The [Neyman-Fisher Factorisation Theorem](#neyman-fisher-factorisation-theorem) shows that if we can factor a probability density or mass function (or likelihood) into a specific form, then we can easily identify the sufficient statistic(s) for the parameters of that probability density or mass function (or the likelihood). This concept of being able to factor distributions into a common form with some similar properties is useful not only for probabilistic modelling but also for classical and Bayesian inference.  

The exponential families of distributions refer to a rather large and ubiquitous set of probability distributions that we can write in a general form. We use the term "families" to denoting each distribution as a "family," i.e.\ the Gaussian distribution is a family of all possible Gaussian distributions and is a member of the exponential families. 
The exponential families and their common form have several useful properties that facilitate many inference tasks, including parameter estimation and inference and model construction and evaluation. 

:::{.sidenote}
**Example:**\
The Pareto distribution is a continuous probability distribution with density function 
$$
f(y;\alpha,y_m)=\frac{\alpha y_m^\alpha}{y^{\alpha+1}},\:y\in[y_m,\infty)
$$
Because the support of $Y$ depends on the parameter $y_m$, the Pareto distribution is not an exponential family. 

:::


## The Exponential Families Common Form

Members of the exponential family of distributions are probability distributions whose support is independent of their parameters and that we can write in a common form
$$
f(y;\boldsymbol{\theta})=h(y)\exp\left\{\eta(\boldsymbol{\theta})\,T(y)-A(\boldsymbol{\theta}
\right)\}.
$$



* $h(y)>0$ is a function of the data
* $T(y)$ is the sufficient statistic.  For the likelihood function with samples $y_1,y_2,\ldots,y_n$ the sufficient statistic is $\sum_{i=1}^nT(y_i)$.  
* If $\eta(\theta)=\theta=\eta$, $\eta$ is called the *natural parameter* and the family is said to be in the canonical form

$$
f(y;\boldsymbol{\theta})=h(y)\exp\left\{\eta\, T(y)-A(\eta)\right\}
$$

* If $\eta(\theta)=\eta$ and $T(y)=x$ the familiy is called a *natural exponential* family, and  the *natural parameter space* is

$$
\{\eta:f(y;\theta)<\infty\}.
$$ 

* The function $A(\theta)$ or $A(\eta)$ is defined after finding the other components and is called the log-partition function because it is the natural log of the normalising constant

$$
A(\eta)=\log\left[
\int_Yh(y)\,exp\left\{\eta(\theta)\,T(y)\right\}dy
\right].
$$
There are numerous examples of exponential families, and the concept of exponential families is fundamental to generalised linear models and Bayesian statistics. 

:::{.boxed}
###   The Gaussian Distribution with Known Variance: {.tabset .tabset-pills}

Consider the Gaussian distribution with known variance $\sigma^2=1$
$$
f(y;\mu)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac12(x-\mu)^2\right)
$$
Show that it can be written as a member of the exponential families.

####    Solution

We can expand the quadratic term for easier factorisation
$$
f(y;\mu)=\frac{1}{\sqrt{2\pi}}\exp\left\{-\frac12\left(y^2-2y\mu+\mu^2\right)\right\}
$$
resulting in:
$$
\begin{align}
h(y)&=  \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{y^2}{2}\right)\\
\eta(\mu)&=\mu\\
T(y)&=y\\
A(\mu)&=\frac{\mu^2}{2}.
\end{align}
$$



:::

:::{.boxed}
###   The Poisson Distribution {.tabset .tabset-pills}

Consider the Poisson Distribution with the probability mass function. 

show that it can be written as a member of the exponential families.

####    Solution 
We write the $\log(\mbox{pmf})$ as
$$
\begin{align}
p(y) &= \frac{\lambda^ye^{-\lambda}}{y!}\\
&=\frac{1}{y!}\exp\left(y\log(\lambda)-\lambda\right)
\end{align}
$$
and can factor it as
$$
\begin{align}
h(y)&=\frac{1}{y!}\\
\eta(\lambda)&=\log(\lambda)\\
T(y)&=y\\
A(\lambda)&=\lambda\:\left(\mbox{or }A(\eta)=e^{\eta}\right)
\end{align}
$$


:::


:::{.boxed}
###   The Binomial Distribution {.tabset .tabset-pills}

Consider the binomial distribution with probability mass function 
$$
p(Y=y;p,n)={n \choose y}p^y(1-p)^{n-y}
$$

show that it can be written as a member of the exponential families.

####    Solution 

This requires some manipulation on of the pmf

$$
\begin{align}
p(y)&={n\choose y}p^y(1-p)^{n-y}\\
&={n\choose y}\exp\left(
y\log\left(\frac{p}{1-p}
\right)+n\log(1-p)
\right)
\end{align}
$$
resulting in
$$
\begin{align}
h(y)&={n\choose y}\\
\eta(p)&=\log\left(\frac{p}{1-p}\right)\\
T(y)&=y\\
A(p)&=-n\log(1-p).
\end{align}
$$




:::

## Deviance 

:::{.sidenote}
**The Saturated Model**\
The saturated model is the model that fits the data "perfectly" typically; this means that there is one parameter for every observation, e.g.\ each observation has a mean equal to the value of the observation. This one-to-one relationship is not strictly true; see the Gaussian case where there are more parameters than observations in the saturated model ($\sigma^2$).

While there is limited use for the saturated model (as it has $0$ degrees of freedom, we can't use it for inference), it is a useful theoretical construct to compare with other models. 
:::

Deviance is a goodness of fit measure for statistical models, comparing a fitted model to a theoretical "best fit" or saturated model. Specifically, it is twice the difference between the log-likelihood evaluated at the saturated parameter values and the log-likelihood evaluated at the proposed model estimated parameter values
$$
D(\mathbf{y},\theta)=2\left\{l(\theta_s|\mathbf{y})-l(\hat{\theta}|\mathbf{y})\right\}
$$
where $\theta_s$ is the saturated model estimates of the parameters, and $\hat{\theta}$ are the proposed model parameter estimates (typically the MLE).

While deviance is a relative measure, because the saturated log-likelihood may not be equal to $0$, it can be useful for comparing different models. 

For the Gaussian model, the deviance reduces to the standard measure of goodness of fit, the sum of squares. 

:::{.sidenote}
**Example:**\
Consider a sample $\mathbf{y}=y_1,y_2,\ldots,y_n$ from 
$$
f(y;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right).
$$
Show that if $\hat{\mu}=\bar{y}$ the deviance $D(\mathbf{y},\mu)=\sum_{i=1}^n(y_i-\bar{y})^2/\sigma^2$.
:::

:::{.boxed}
###   Deviance for a Gaussian Distribution {.tabset .tabset-pills}

Consider a sample $\mathbf{y}=y_1,y_2,\ldots,y_n$ from 
$$
f(y;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right).
$$
Show that if $\hat{\mu}=\bar{y}$ the deviance $D(\mathbf{y},\mu)=\sum_{i=1}^n(y_i-\bar{y})^2/\sigma^2$.

#### Solution 
For a sample $\mathbf{y}=y_1,y_2,\ldots,y_n$ from 
$$
f(y;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right).
$$
Assume that the saturated values for $\mu$ are $\mu_i=y_i$. The deviance is then
$$
\begin{align}
D(\mathbf{y},\hat{\mu})&=2\left\{l(\mu_s|\mathbf{y})-l(\hat{\mu}|\mathbf{y})\right\}\\
&=2\left\{-\frac{n}{2}\log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-y_i)^2+\frac{n}{2}\log(2\pi\sigma^2)+\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\bar{y})^2\right\}\\
&=\frac{1}{\sigma^2}\sum_{i=1}^n(y_i-\bar{y})^2.
\end{align}
$$

:::

The sum of squares measure of goodness of fit arises naturally when considering the Gaussian distribution; the deviance allows us to extend the same derivation to non-Gaussian distributions, providing a general framework for measuring and comparing model goodness of fit. 

:::{.sidenote}
**Example:**\
For a sample $\mathbf{y}=y_1,y_2,\ldots,y_n$ from 
$$
p(y)=\frac{\lambda^{y}e^{-\lambda}}{y!}
$$
find the deviance.
:::



:::{.boxed}
###   Poisson Deviance {.tabset .tabset-pills}

For a sample $\mathbf{y}=y_1,y_2,\ldots,y_n$ from 
$$
p(y)=\frac{\lambda^{y}e^{-\lambda}}{y!}
$$
find the deviance.

####    Solution 

For a sample $\mathbf{y}=y_1,y_2,\ldots,y_n$ from 
$$
p(y)=\frac{\lambda^{y}e^{-\lambda}}{y!}
$$
the saturated values are $\lambda_i=y_i$, then we can write the deviance as:
$$
\begin{align}
D(\mathbf{y},\hat{\lambda})&=2\left\{l(\lambda_s|\mathbf{y})-l(\hat{\lambda}|\mathbf{y})\right\}\\
&=2\left\{\sum_{i=1}^ny_i\log(y_i)-y_i-\log(y_i!)-\sum_{i=1}^ny_i\log(\hat{\lambda})-\hat{\lambda}-\log(y_i)\right\}\\
&=2\left\{\sum_{i=1}^ny_i\left(\frac{y_i}{\hat{\lambda}}\right)-(y_i-\hat{\lambda})
\right\}
\end{align}
$$



:::


## Information Criteria

:::{.sidenote}
**Example:**\
Given a set of $n=30$ observations $\mathbf{y}=y_1,y_2,\ldots,y_n$ these data were generated from one of two distributions
$$
Pr(Y=y)=p^y(1-p)^{1-y}
$$
or
$$
Pr(Y=y)=\frac{\lambda^ye^{-\lambda}}{y!}.
$$
In either case the sufficient statistic is $T(\mathbf{y})=\sum_{i=1}^ny_i=4$.  

Compute the AIC and the BIC for both possible models.

$\left(\mathbf{Hint\!:}\  \sum_{i=1}^n\log(y_i!)=0\right)$


:::

In an [information theoretic](https://en.wikipedia.org/wiki/Information_theory) framework, a statistical model represents some processes that represent data because the model is imperfect, there is some information loss in choosing a model. From this perspective, we can interpret the ideas inherent in deviance (i.e.\ the comparison of a log-likelihood function evaluated under differing assumptions) in this framework.  To compare information loss between competing models, we measure model goodness of fit at its optimum (e.g. \ the log-likelihood evaluated at the MLE).  Because we can assume that fit improves with model complexity (i.e.\ as the model approaches the saturated model), it is important to avoid over-fitting or choosing an overly complex model with poor predictive performance.  Avoiding over-fitting can be addressed by selecting some penalty on model complexity (e.g.\ the adjusted $R^2$ in linear regression is an example of this). We can use information criteria such as this for more than just comparing model goodness of fit, but provide a basis for model selection or choosing the preferred model as a (possible) compromise between fit and complexity. 

### Akaike Information Criteria
The Akaike Information Criteria (AIC) estimates the predictive error for a model that includes a penalty term based on $k$ the number of estimated parameters in the model. The (AIC) is twice the number of estimated parameters minus twice the maximum of the log-likelihood (i.e.\ evaluated at the MLE).

$$
\begin{align}
\operatorname{AIC} &= 2k-2l(\hat{\theta}|\mathbf{y}).
\end{align}
$$
The AIC is a relative measure, not absolute and is only useful in comparing two (or more) competing models.  There is no guarantee that the best model (the one with the smallest AIC) of a set of models is the overall best model (there may be other unknown or untested models that are better).  

### Bayesian Information Criteria

The Bayesian Information Criteria (BIC) is similar to the AIC, differing on the surface only in penalising model complexity.  The actual derivation of the AIC and BIC differ greatly.  The AIC is derived from an information-theoretic perspective based on an asymptotic approximation of the Kullbeck-Liebler divergence. The BIC is derived from a Taylor series expansion of the log-likelihood conditioned on the model (averaged over the parameter space $\Theta$). Both of these derivations are not presented here to focus on the use and comparison of the two criteria.  

Given a sample $\mathbf{y}=y_1,y_2,\ldots,y_n$ the Bayesian Information Criteria is
$$
\operatorname{BIC}=k\log(n)-2l(\hat{\theta}|\mathbf{y})
$$
The BIC typically penalises model complexity more than the AIC and is generally comparable to the AIC.  However, there are a few distinctions and caveats to the BIC.  To use the BIC $n>>p$, there must be many more observations than estimated model parameters, and typically BIC does not work well for high-dimensioned variable selection problems. 

The BIC is used much the same way as the AIC for comparing two competing models, but it doesn't have the same useful interpretation via the $\chi^2$ distribution as the AIC. Instead, there are some heuristic rules of thumb for interpreting the strength of evidence for differences in BIC.


:::{.table}
|Difference in BIC | Strength of Evidence Against Higher BIC |
|------------------|-----------------------------------------|
|0 to 2 | Not worth mention |
|2 to 6 | Positive |
|6 to 10 | Strong |
|>10 | Very Strong |

:::


:::{.boxed}
###   Finding the AIC and BIC {.tabset .tabset-pills}

Given a set of $n=30$ observations $\mathbf{y}=y_1,y_2,\ldots,y_n$ these data were generated from one of two distributions
$$
Pr(Y=y)=p^y(1-p)^{1-y}
$$
or
$$
Pr(Y=y)=\frac{\lambda^ye^{-\lambda}}{y!}.
$$
In either case the sufficient statistic is $T(\mathbf{y})=\sum_{i=1}^ny_i=4$.  

Compute the AIC and the BIC for both possible models.

$\left(\mathbf{Hint\!:}\  \sum_{i=1}^n\log(y_i!)=0\right)$

####    Solution 

**Case 1: The Bernoulli Distribution**

$$
\begin{align}
\operatorname{AIC} &= 2k-2l(\hat{p}|\mathbf{y})\\
&=2(1)-2\left\{\sum_{i=1}^ny_i\log(\hat{p})+\left(n-\sum_{i=1}^ny_i\right)\log(1-\hat{p})\right\}\\
&=2-2\left\{\sum_{i=1}^ny_i\log\left(\frac{\hat{p}}{1-\hat{p}}\right)+n\log(1-\hat{p})\right\}\\
\end{align}
$$
noting that $\hat{p}=4/30$, $1-\hat{p}=26/30$ and $\hat{p}/(1-\hat{p})=4/26$
$$
\begin{align}
\operatorname{AIC}&=2-2\left\{\sum_{i=1}^ny_i\log\left(\frac{\hat{p}}{1-\hat{p}}\right)+n\log(1-\hat{p})\right\}\\
&=2-2\left\{4\log\left(\frac{4}{26}\right)+30\log\left(\frac{26}{30}\right)\right\}\\
&=25.56.
\end{align}
$$
The BIC is 
$\require{cancel}$
$$
\begin{align}
\operatorname{BIC}&=k\log(n)-2l(\hat{p}|\mathbf{y})\\
&=1\log(30)-2\left\{4\log\left(\frac{4}{26}\right)+30\log\left(\frac{26}{30}\right)\right\}\\
&=26.96.
\end{align}
$$
**Case 2: The Poisson Distribution**

$$
\begin{align}
\operatorname{AIC} &= 2k-2l(\hat{p}|\mathbf{y})\\
&=2(1)-2\left\{\sum_{i=1}^ny_i\log(\hat{\lambda})-n\hat{\lambda}-\sum_{i=1}^n\log(y_i!)\right\}
\end{align}
$$
noting that $\hat{\lambda}=4/30$,
$$
\begin{align}
\operatorname{AIC} &= 2k-2l(\hat{p}|\mathbf{y})\\
&=2(1)-2\left\{\sum_{i=1}^ny_i\log(\hat{\lambda})-n\hat{\lambda}-\sum_{i=1}^n\log(y_i!)\right\}\\
&=2-2\left\{4\log\left(\frac{4}{30}\right)-\cancel{30}\frac{4}{\cancel{30}}-\cancelto{0}{\sum_{i=1}^n\log(y_i!)}\right\}\\
&=2-2\left\{4\log\left(\frac{4}{30}\right)-4\right\}\\
&=26.12.
\end{align}
$$
The BIC is 
$$
\begin{align}
\operatorname{BIC}&=n\log(k)-2l(\hat{p}|\mathbf{y})\\
&=1\log(30)-2\left\{4\log\left(\frac{4}{30}\right)-4\right\}\\
&=27.52
\end{align}
$$
The AIC and BIC are different for the two proposed models, indicating some preference for the Bernoulli model. We can interpret this difference for the BIC as "Not worth a mention".  In other words, while there is a difference, the evidence is not compelling. 

**BONUS:** The data were generated from a Bernoulli distribution. 



:::









